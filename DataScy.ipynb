{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv/JBMoZnLJqsRPzDoBBrn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaixLina/Create-Gify-easy-with-direct-yt-link-local-files/blob/main/DataScy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-A5HlNdiao2l",
        "outputId": "6808271f-fc9a-4a83-f118-5f4f602f132c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            URL_ID                                                URL\n",
              "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
              "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
              "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
              "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
              "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa66e4b5-4c0e-45f2-91b0-eaa700312393\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL_ID</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>blackassign0001</td>\n",
              "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>blackassign0002</td>\n",
              "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>blackassign0003</td>\n",
              "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>blackassign0004</td>\n",
              "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>blackassign0005</td>\n",
              "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa66e4b5-4c0e-45f2-91b0-eaa700312393')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fa66e4b5-4c0e-45f2-91b0-eaa700312393 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fa66e4b5-4c0e-45f2-91b0-eaa700312393');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e32f3531-e089-4cbf-9c4a-5c5edd5e27f0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e32f3531-e089-4cbf-9c4a-5c5edd5e27f0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e32f3531-e089-4cbf-9c4a-5c5edd5e27f0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "input_df",
              "summary": "{\n  \"name\": \"input_df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"URL_ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"blackassign0084\",\n          \"blackassign0054\",\n          \"blackassign0071\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"https://insights.blackcoffer.com/how-voice-search-makes-your-business-a-successful-business/\",\n          \"https://insights.blackcoffer.com/how-google-fit-measure-heart-and-respiratory-rates-using-a-phone/\",\n          \"https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes-2/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the input data\n",
        "input_file_path = '/mnt/data/Input.xlsx'\n",
        "input_df = pd.read_excel(\"/content/Input.xlsx\")\n",
        "\n",
        "# Display the contents of the input file\n",
        "input_df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Directory to save the articles\n",
        "output_dir = '/mnt/data/articles'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Extract the article title and text (customize this according to the website structure)\n",
        "            title = soup.find('h1').get_text(strip=True)\n",
        "            paragraphs = soup.find_all('p')\n",
        "            article_text = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            return title, article_text\n",
        "        else:\n",
        "            print(f\"Failed to retrieve the article: {url}\")\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while extracting {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Loop through the URLs and save the extracted articles\n",
        "for index, row in input_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    title, article_text = extract_article_text(url)\n",
        "\n",
        "    if title and article_text:\n",
        "        with open(os.path.join(output_dir, f\"{url_id}.txt\"), 'w', encoding='utf-8') as file:\n",
        "            file.write(f\"{title}\\n\\n{article_text}\")\n",
        "\n",
        "print(\"Article extraction completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6p0BTXdbBth",
        "outputId": "76b8c9c0-170e-4fb8-a26e-6688ede6079c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve the article: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Failed to retrieve the article: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Article extraction completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Directory to save the articles\n",
        "output_dir = '/mnt/data/articles'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Extract the article title and text (customize this according to the website structure)\n",
        "            title = soup.find('h1').get_text(strip=True)\n",
        "            paragraphs = soup.find_all('p')\n",
        "            article_text = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            return title, article_text\n",
        "        else:\n",
        "            print(f\"Failed to retrieve the article: {url}\")\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while extracting {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Loop through the URLs and save the extracted articles\n",
        "for index, row in input_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    title, article_text = extract_article_text(url)\n",
        "\n",
        "    if title and article_text:\n",
        "        with open(os.path.join(output_dir, f\"{url_id}.txt\"), 'w', encoding='utf-8') as file:\n",
        "            file.write(f\"{title}\\n\\n{article_text}\")\n",
        "\n",
        "print(\"Article extraction completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dDBvq8Rbigs",
        "outputId": "c9ec4049-7008-4f63-fc62-39e2cd0f84b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve the article: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Failed to retrieve the article: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Article extraction completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgdW0T5DesCB",
        "outputId": "f33df23f-9bac-47d0-e747-7612724635ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.15.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import textstat\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Directory containing the extracted articles\n",
        "input_dir = '/mnt/data/articles'\n",
        "\n",
        "# Function to compute textual analysis metrics\n",
        "def analyze_text(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Compute metrics\n",
        "    word_count = len(words)\n",
        "    sentence_count = len(sentences)\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count != 0 else 0\n",
        "    lexical_diversity = len(set(words)) / word_count if word_count != 0 else 0\n",
        "    sentiment_scores = sia.polarity_scores(text)\n",
        "    readability_fk_grade = textstat.flesch_kincaid_grade(text)\n",
        "    readability_fk_ease = textstat.flesch_reading_ease(text)\n",
        "\n",
        "    return {\n",
        "        'word_count': word_count,\n",
        "        'sentence_count': sentence_count,\n",
        "        'avg_sentence_length': avg_sentence_length,\n",
        "        'lexical_diversity': lexical_diversity,\n",
        "        'sentiment_scores': sentiment_scores,\n",
        "        'readability_fk_grade': readability_fk_grade,\n",
        "        'readability_fk_ease': readability_fk_ease\n",
        "    }\n",
        "\n",
        "# Loop through the extracted articles and perform analysis\n",
        "analysis_results = []\n",
        "\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(input_dir, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            title = file.readline().strip()\n",
        "            article_text = file.read().strip()\n",
        "            analysis = analyze_text(article_text)\n",
        "            analysis['title'] = title\n",
        "            analysis['filename'] = filename\n",
        "            analysis_results.append(analysis)\n",
        "\n",
        "# Print or save the analysis results\n",
        "for result in analysis_results:\n",
        "    print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-p63oy3dEGh",
        "outputId": "d364feab-2fd8-430d-f8ed-1df093216b15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'word_count': 852, 'sentence_count': 25, 'avg_sentence_length': 34.08, 'lexical_diversity': 0.5434272300469484, 'sentiment_scores': {'neg': 0.014, 'neu': 0.854, 'pos': 0.131, 'compound': 0.9976}, 'readability_fk_grade': 16.3, 'readability_fk_ease': 26.74, 'title': 'Rise of OTT platform and its impact on entertainment industry by the year 2030', 'filename': 'blackassign0024.txt'}\n",
            "{'word_count': 389, 'sentence_count': 11, 'avg_sentence_length': 35.36363636363637, 'lexical_diversity': 0.6401028277634961, 'sentiment_scores': {'neg': 0.008, 'neu': 0.892, 'pos': 0.1, 'compound': 0.9823}, 'readability_fk_grade': 18.5, 'readability_fk_ease': 21.16, 'title': 'Human Rights Outlook', 'filename': 'blackassign0083.txt'}\n",
            "{'word_count': 1550, 'sentence_count': 61, 'avg_sentence_length': 25.40983606557377, 'lexical_diversity': 0.46580645161290324, 'sentiment_scores': {'neg': 0.077, 'neu': 0.862, 'pos': 0.061, 'compound': -0.9741}, 'readability_fk_grade': 13.2, 'readability_fk_ease': 40.38, 'title': 'Impact of COVID-19 pandemic on Tourism & Aviation industries', 'filename': 'blackassign0079.txt'}\n",
            "{'word_count': 1506, 'sentence_count': 67, 'avg_sentence_length': 22.47761194029851, 'lexical_diversity': 0.47742363877822047, 'sentiment_scores': {'neg': 0.116, 'neu': 0.777, 'pos': 0.107, 'compound': -0.9745}, 'readability_fk_grade': 10.9, 'readability_fk_ease': 51.68, 'title': 'Gaming Disorder and Effects of Gaming on Health.', 'filename': 'blackassign0094.txt'}\n",
            "{'word_count': 1387, 'sentence_count': 54, 'avg_sentence_length': 25.685185185185187, 'lexical_diversity': 0.4671953857245854, 'sentiment_scores': {'neg': 0.094, 'neu': 0.804, 'pos': 0.102, 'compound': 0.8944}, 'readability_fk_grade': 14.2, 'readability_fk_ease': 32.33, 'title': 'Negative effects of marketing on society', 'filename': 'blackassign0021.txt'}\n",
            "{'word_count': 1563, 'sentence_count': 77, 'avg_sentence_length': 20.2987012987013, 'lexical_diversity': 0.4113883557261676, 'sentiment_scores': {'neg': 0.099, 'neu': 0.778, 'pos': 0.123, 'compound': 0.9692}, 'readability_fk_grade': 10.3, 'readability_fk_ease': 53.21, 'title': 'Lessons from the past: Some key learnings relevant to the coronavirus crisis', 'filename': 'blackassign0090.txt'}\n",
            "{'word_count': 1427, 'sentence_count': 42, 'avg_sentence_length': 33.976190476190474, 'lexical_diversity': 0.373510861948143, 'sentiment_scores': {'neg': 0.085, 'neu': 0.818, 'pos': 0.097, 'compound': 0.8857}, 'readability_fk_grade': 15.3, 'readability_fk_ease': 40.31, 'title': 'Impact of COVID-19 pandemic on office space and co-working industries.', 'filename': 'blackassign0097.txt'}\n",
            "{'word_count': 930, 'sentence_count': 39, 'avg_sentence_length': 23.846153846153847, 'lexical_diversity': 0.5365591397849462, 'sentiment_scores': {'neg': 0.066, 'neu': 0.864, 'pos': 0.07, 'compound': 0.5146}, 'readability_fk_grade': 12.7, 'readability_fk_ease': 41.6, 'title': 'What is the repercussion of the environment due to the COVID-19 pandemic situation?', 'filename': 'blackassign0095.txt'}\n",
            "{'word_count': 1750, 'sentence_count': 60, 'avg_sentence_length': 29.166666666666668, 'lexical_diversity': 0.42514285714285716, 'sentiment_scores': {'neg': 0.093, 'neu': 0.834, 'pos': 0.073, 'compound': -0.9937}, 'readability_fk_grade': 14.6, 'readability_fk_ease': 36.63, 'title': 'Environmental impact of the COVID-19 pandemic – Lesson for the Future', 'filename': 'blackassign0050.txt'}\n",
            "{'word_count': 687, 'sentence_count': 31, 'avg_sentence_length': 22.161290322580644, 'lexical_diversity': 0.5356622998544396, 'sentiment_scores': {'neg': 0.058, 'neu': 0.849, 'pos': 0.093, 'compound': 0.9523}, 'readability_fk_grade': 12.0, 'readability_fk_ease': 43.32, 'title': 'How advertisement/marketing affects business.', 'filename': 'blackassign0022.txt'}\n",
            "{'word_count': 1379, 'sentence_count': 81, 'avg_sentence_length': 17.02469135802469, 'lexical_diversity': 0.43944887599709936, 'sentiment_scores': {'neg': 0.163, 'neu': 0.686, 'pos': 0.151, 'compound': -0.9798}, 'readability_fk_grade': 8.8, 'readability_fk_ease': 57.16, 'title': 'How to Overcome Your Fear of Making Mistakes', 'filename': 'blackassign0071.txt'}\n",
            "{'word_count': 605, 'sentence_count': 29, 'avg_sentence_length': 20.862068965517242, 'lexical_diversity': 0.47768595041322315, 'sentiment_scores': {'neg': 0.005, 'neu': 0.868, 'pos': 0.127, 'compound': 0.9957}, 'readability_fk_grade': 11.8, 'readability_fk_ease': 43.83, 'title': 'Rising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040.', 'filename': 'blackassign0001.txt'}\n",
            "{'word_count': 394, 'sentence_count': 9, 'avg_sentence_length': 43.77777777777778, 'lexical_diversity': 0.5862944162436549, 'sentiment_scores': {'neg': 0.008, 'neu': 0.918, 'pos': 0.074, 'compound': 0.9638}, 'readability_fk_grade': 22.9, 'readability_fk_ease': 4.18, 'title': 'Contribution of handicrafts (Visual Arts & Literature) in the Indian economy', 'filename': 'blackassign0098.txt'}\n",
            "{'word_count': 1985, 'sentence_count': 85, 'avg_sentence_length': 23.352941176470587, 'lexical_diversity': 0.4181360201511335, 'sentiment_scores': {'neg': 0.025, 'neu': 0.82, 'pos': 0.155, 'compound': 0.9997}, 'readability_fk_grade': 12.3, 'readability_fk_ease': 42.72, 'title': 'Future of Work: How AI Has Entered the Workplace', 'filename': 'blackassign0043.txt'}\n",
            "{'word_count': 1633, 'sentence_count': 85, 'avg_sentence_length': 19.211764705882352, 'lexical_diversity': 0.4329454990814452, 'sentiment_scores': {'neg': 0.058, 'neu': 0.812, 'pos': 0.13, 'compound': 0.9987}, 'readability_fk_grade': 10.8, 'readability_fk_ease': 46.57, 'title': 'How Robo Human will Impact the Future?', 'filename': 'blackassign0041.txt'}\n",
            "{'word_count': 1775, 'sentence_count': 71, 'avg_sentence_length': 25.0, 'lexical_diversity': 0.38422535211267606, 'sentiment_scores': {'neg': 0.036, 'neu': 0.84, 'pos': 0.124, 'compound': 0.9992}, 'readability_fk_grade': 15.6, 'readability_fk_ease': 23.36, 'title': 'Rise of Internet Demand and its Impact on Communications and Alternatives by the Year 2035.', 'filename': 'blackassign0011.txt'}\n",
            "{'word_count': 982, 'sentence_count': 35, 'avg_sentence_length': 28.057142857142857, 'lexical_diversity': 0.5173116089613035, 'sentiment_scores': {'neg': 0.028, 'neu': 0.842, 'pos': 0.131, 'compound': 0.9978}, 'readability_fk_grade': 14.0, 'readability_fk_ease': 32.83, 'title': 'Management challenges for future digitalization of healthcare services', 'filename': 'blackassign0061.txt'}\n",
            "{'word_count': 2015, 'sentence_count': 74, 'avg_sentence_length': 27.22972972972973, 'lexical_diversity': 0.3945409429280397, 'sentiment_scores': {'neg': 0.022, 'neu': 0.849, 'pos': 0.129, 'compound': 0.9996}, 'readability_fk_grade': 13.5, 'readability_fk_ease': 39.47, 'title': 'How does artificial intelligence affect the environment', 'filename': 'blackassign0070.txt'}\n",
            "{'word_count': 1034, 'sentence_count': 29, 'avg_sentence_length': 35.6551724137931, 'lexical_diversity': 0.5058027079303675, 'sentiment_scores': {'neg': 0.029, 'neu': 0.871, 'pos': 0.1, 'compound': 0.9941}, 'readability_fk_grade': 17.1, 'readability_fk_ease': 30.03, 'title': 'Impacts of COVID 19 on Vegetable Vendors', 'filename': 'blackassign0078.txt'}\n",
            "{'word_count': 1435, 'sentence_count': 56, 'avg_sentence_length': 25.625, 'lexical_diversity': 0.4355400696864111, 'sentiment_scores': {'neg': 0.128, 'neu': 0.769, 'pos': 0.103, 'compound': -0.9906}, 'readability_fk_grade': 16.7, 'readability_fk_ease': 14.9, 'title': 'Rise of Cybercrime and its Effect in upcoming Future', 'filename': 'blackassign0004.txt'}\n",
            "{'word_count': 2196, 'sentence_count': 78, 'avg_sentence_length': 28.153846153846153, 'lexical_diversity': 0.40209471766848814, 'sentiment_scores': {'neg': 0.044, 'neu': 0.901, 'pos': 0.055, 'compound': 0.4044}, 'readability_fk_grade': 14.1, 'readability_fk_ease': 37.94, 'title': 'Will we ever understand the nature of consciousness?', 'filename': 'blackassign0064.txt'}\n",
            "{'word_count': 2638, 'sentence_count': 85, 'avg_sentence_length': 31.03529411764706, 'lexical_diversity': 0.3612585291887794, 'sentiment_scores': {'neg': 0.044, 'neu': 0.813, 'pos': 0.143, 'compound': 0.9998}, 'readability_fk_grade': 13.7, 'readability_fk_ease': 44.48, 'title': 'Deep learning impact on areas of e-learning?', 'filename': 'blackassign0038.txt'}\n",
            "{'word_count': 2138, 'sentence_count': 95, 'avg_sentence_length': 22.50526315789474, 'lexical_diversity': 0.40505144995322734, 'sentiment_scores': {'neg': 0.017, 'neu': 0.829, 'pos': 0.154, 'compound': 0.9998}, 'readability_fk_grade': 10.8, 'readability_fk_ease': 51.89, 'title': 'How to increase social media engagement for marketers?', 'filename': 'blackassign0086.txt'}\n",
            "{'word_count': 1436, 'sentence_count': 54, 'avg_sentence_length': 26.59259259259259, 'lexical_diversity': 0.4561281337047354, 'sentiment_scores': {'neg': 0.132, 'neu': 0.791, 'pos': 0.077, 'compound': -0.9982}, 'readability_fk_grade': 14.0, 'readability_fk_ease': 38.15, 'title': 'Due to the COVID-19 the repercussion of the environment', 'filename': 'blackassign0096.txt'}\n",
            "{'word_count': 1431, 'sentence_count': 49, 'avg_sentence_length': 29.20408163265306, 'lexical_diversity': 0.4528301886792453, 'sentiment_scores': {'neg': 0.021, 'neu': 0.876, 'pos': 0.103, 'compound': 0.9977}, 'readability_fk_grade': 14.9, 'readability_fk_ease': 30.4, 'title': 'Rise of e-health and its impact on humans by the year 2030', 'filename': 'blackassign0018.txt'}\n",
            "{'word_count': 1585, 'sentence_count': 63, 'avg_sentence_length': 25.158730158730158, 'lexical_diversity': 0.45678233438485805, 'sentiment_scores': {'neg': 0.051, 'neu': 0.757, 'pos': 0.192, 'compound': 0.9997}, 'readability_fk_grade': 12.9, 'readability_fk_ease': 41.09, 'title': 'Rise of telemedicine and its Impact on Livelihood by 2040', 'filename': 'blackassign0015.txt'}\n",
            "{'word_count': 816, 'sentence_count': 29, 'avg_sentence_length': 28.137931034482758, 'lexical_diversity': 0.3946078431372549, 'sentiment_scores': {'neg': 0.009, 'neu': 0.781, 'pos': 0.21, 'compound': 0.9994}, 'readability_fk_grade': 14.3, 'readability_fk_ease': 37.34, 'title': 'How advertisement increase your market value?', 'filename': 'blackassign0020.txt'}\n",
            "{'word_count': 1643, 'sentence_count': 60, 'avg_sentence_length': 27.383333333333333, 'lexical_diversity': 0.44004869141813757, 'sentiment_scores': {'neg': 0.035, 'neu': 0.807, 'pos': 0.159, 'compound': 0.9996}, 'readability_fk_grade': 13.7, 'readability_fk_ease': 39.06, 'title': 'Will machine replace the human in the future of work?', 'filename': 'blackassign0034.txt'}\n",
            "{'word_count': 1290, 'sentence_count': 51, 'avg_sentence_length': 25.294117647058822, 'lexical_diversity': 0.46434108527131784, 'sentiment_scores': {'neg': 0.095, 'neu': 0.815, 'pos': 0.09, 'compound': -0.8917}, 'readability_fk_grade': 12.7, 'readability_fk_ease': 41.5, 'title': 'COVID-19 Impact on Hospitality Industry', 'filename': 'blackassign0089.txt'}\n",
            "{'word_count': 1275, 'sentence_count': 63, 'avg_sentence_length': 20.238095238095237, 'lexical_diversity': 0.46980392156862744, 'sentiment_scores': {'neg': 0.121, 'neu': 0.794, 'pos': 0.085, 'compound': -0.9957}, 'readability_fk_grade': 13.4, 'readability_fk_ease': 34.46, 'title': 'Rise of Cyber Crime and its Effects', 'filename': 'blackassign0007.txt'}\n",
            "{'word_count': 4197, 'sentence_count': 199, 'avg_sentence_length': 21.09045226130653, 'lexical_diversity': 0.2740052418394091, 'sentiment_scores': {'neg': 0.078, 'neu': 0.827, 'pos': 0.095, 'compound': 0.9986}, 'readability_fk_grade': 8.4, 'readability_fk_ease': 63.7, 'title': 'Impact of COVID-19 pandemic on sports events around the world.', 'filename': 'blackassign0080.txt'}\n",
            "{'word_count': 1984, 'sentence_count': 84, 'avg_sentence_length': 23.61904761904762, 'lexical_diversity': 0.2963709677419355, 'sentiment_scores': {'neg': 0.118, 'neu': 0.732, 'pos': 0.15, 'compound': 0.9914}, 'readability_fk_grade': 13.8, 'readability_fk_ease': 33.44, 'title': 'Rise of Cybercrime and its Effect by the Year 2040.', 'filename': 'blackassign0010.txt'}\n",
            "{'word_count': 1074, 'sentence_count': 50, 'avg_sentence_length': 21.48, 'lexical_diversity': 0.4590316573556797, 'sentiment_scores': {'neg': 0.005, 'neu': 0.863, 'pos': 0.132, 'compound': 0.9988}, 'readability_fk_grade': 11.8, 'readability_fk_ease': 44.03, 'title': 'All you need to know about online marketing', 'filename': 'blackassign0046.txt'}\n",
            "{'word_count': 2020, 'sentence_count': 65, 'avg_sentence_length': 31.076923076923077, 'lexical_diversity': 0.3589108910891089, 'sentiment_scores': {'neg': 0.034, 'neu': 0.829, 'pos': 0.137, 'compound': 0.9997}, 'readability_fk_grade': 14.0, 'readability_fk_ease': 43.66, 'title': 'How AI will change the World?', 'filename': 'blackassign0042.txt'}\n",
            "{'word_count': 632, 'sentence_count': 20, 'avg_sentence_length': 31.6, 'lexical_diversity': 0.5506329113924051, 'sentiment_scores': {'neg': 0.003, 'neu': 0.894, 'pos': 0.103, 'compound': 0.9947}, 'readability_fk_grade': 15.8, 'readability_fk_ease': 33.58, 'title': 'How Google fit measure heart and respiratory rates using a phone camera?', 'filename': 'blackassign0054.txt'}\n",
            "{'word_count': 1403, 'sentence_count': 77, 'avg_sentence_length': 18.22077922077922, 'lexical_diversity': 0.44689950106913756, 'sentiment_scores': {'neg': 0.162, 'neu': 0.688, 'pos': 0.15, 'compound': -0.9796}, 'readability_fk_grade': 9.2, 'readability_fk_ease': 56.15, 'title': 'How to overcome your fear of making mistakes?', 'filename': 'blackassign0075.txt'}\n",
            "{'word_count': 654, 'sentence_count': 31, 'avg_sentence_length': 21.096774193548388, 'lexical_diversity': 0.518348623853211, 'sentiment_scores': {'neg': 0.026, 'neu': 0.811, 'pos': 0.163, 'compound': 0.9981}, 'readability_fk_grade': 11.6, 'readability_fk_ease': 44.54, 'title': 'What patients like and dislike about telemedicine?', 'filename': 'blackassign0057.txt'}\n",
            "{'word_count': 1548, 'sentence_count': 83, 'avg_sentence_length': 18.650602409638555, 'lexical_diversity': 0.374031007751938, 'sentiment_scores': {'neg': 0.055, 'neu': 0.8, 'pos': 0.146, 'compound': 0.9993}, 'readability_fk_grade': 8.5, 'readability_fk_ease': 63.29, 'title': 'Is Perfection the Greatest enemy of Productivity?', 'filename': 'blackassign0072.txt'}\n",
            "{'word_count': 1314, 'sentence_count': 61, 'avg_sentence_length': 21.540983606557376, 'lexical_diversity': 0.4444444444444444, 'sentiment_scores': {'neg': 0.092, 'neu': 0.762, 'pos': 0.146, 'compound': 0.9957}, 'readability_fk_grade': 17.6, 'readability_fk_ease': 7.35, 'title': 'Rise of Cybercrime and its Effect by the Year 2040.', 'filename': 'blackassign0009.txt'}\n",
            "{'word_count': 1727, 'sentence_count': 67, 'avg_sentence_length': 25.776119402985074, 'lexical_diversity': 0.4047481181239143, 'sentiment_scores': {'neg': 0.034, 'neu': 0.832, 'pos': 0.133, 'compound': 0.9994}, 'readability_fk_grade': 13.0, 'readability_fk_ease': 40.79, 'title': 'Rising IT cities will impact the economy, environment, infrastructure, and city life by the year 2035', 'filename': 'blackassign0023.txt'}\n",
            "{'word_count': 2126, 'sentence_count': 73, 'avg_sentence_length': 29.123287671232877, 'lexical_diversity': 0.3951081843838194, 'sentiment_scores': {'neg': 0.041, 'neu': 0.845, 'pos': 0.113, 'compound': 0.9992}, 'readability_fk_grade': 12.7, 'readability_fk_ease': 52.33, 'title': 'Will we ever colonize outer space?', 'filename': 'blackassign0065.txt'}\n",
            "{'word_count': 2140, 'sentence_count': 87, 'avg_sentence_length': 24.597701149425287, 'lexical_diversity': 0.3981308411214953, 'sentiment_scores': {'neg': 0.038, 'neu': 0.89, 'pos': 0.071, 'compound': 0.9929}, 'readability_fk_grade': 11.9, 'readability_fk_ease': 49.04, 'title': 'Changing landscape and emerging trends in the Indian IT/ITeS Industry.', 'filename': 'blackassign0081.txt'}\n",
            "{'word_count': 1432, 'sentence_count': 73, 'avg_sentence_length': 19.616438356164384, 'lexical_diversity': 0.43645251396648044, 'sentiment_scores': {'neg': 0.042, 'neu': 0.831, 'pos': 0.127, 'compound': 0.9987}, 'readability_fk_grade': 11.0, 'readability_fk_ease': 45.96, 'title': 'Rise of e-health and its impact on humans by the year 2030', 'filename': 'blackassign0014.txt'}\n",
            "{'word_count': 1061, 'sentence_count': 49, 'avg_sentence_length': 21.653061224489797, 'lexical_diversity': 0.43826578699340246, 'sentiment_scores': {'neg': 0.022, 'neu': 0.877, 'pos': 0.101, 'compound': 0.9969}, 'readability_fk_grade': 11.6, 'readability_fk_ease': 44.44, 'title': 'How humans and machines are evolving to work together?', 'filename': 'blackassign0035.txt'}\n",
            "{'word_count': 2059, 'sentence_count': 84, 'avg_sentence_length': 24.511904761904763, 'lexical_diversity': 0.2627489072365226, 'sentiment_scores': {'neg': 0.019, 'neu': 0.851, 'pos': 0.129, 'compound': 0.9994}, 'readability_fk_grade': 12.6, 'readability_fk_ease': 41.8, 'title': 'Rise of telemedicine and its Impact on Livelihood by 2040', 'filename': 'blackassign0012.txt'}\n",
            "{'word_count': 1395, 'sentence_count': 42, 'avg_sentence_length': 33.214285714285715, 'lexical_diversity': 0.45734767025089607, 'sentiment_scores': {'neg': 0.055, 'neu': 0.83, 'pos': 0.114, 'compound': 0.9978}, 'readability_fk_grade': 15.8, 'readability_fk_ease': 33.48, 'title': 'Estimating the impact of COVID-19 on the world of work', 'filename': 'blackassign0091.txt'}\n",
            "{'word_count': 1171, 'sentence_count': 60, 'avg_sentence_length': 19.516666666666666, 'lexical_diversity': 0.4227156276686593, 'sentiment_scores': {'neg': 0.134, 'neu': 0.738, 'pos': 0.128, 'compound': -0.939}, 'readability_fk_grade': 9.8, 'readability_fk_ease': 54.42, 'title': 'Can robots tackle late-life loneliness?', 'filename': 'blackassign0059.txt'}\n",
            "{'word_count': 881, 'sentence_count': 29, 'avg_sentence_length': 30.379310344827587, 'lexical_diversity': 0.40408626560726446, 'sentiment_scores': {'neg': 0.011, 'neu': 0.831, 'pos': 0.158, 'compound': 0.9988}, 'readability_fk_grade': 16.7, 'readability_fk_ease': 25.83, 'title': 'Rise of e-health and its impact on humans by the year 2030', 'filename': 'blackassign0013.txt'}\n",
            "{'word_count': 1232, 'sentence_count': 45, 'avg_sentence_length': 27.377777777777776, 'lexical_diversity': 0.49594155844155846, 'sentiment_scores': {'neg': 0.031, 'neu': 0.884, 'pos': 0.086, 'compound': 0.9922}, 'readability_fk_grade': 12.6, 'readability_fk_ease': 41.9, 'title': 'Rise of electric vehicle and its impact on livelihood by the year 2040.', 'filename': 'blackassign0026.txt'}\n",
            "{'word_count': 1410, 'sentence_count': 51, 'avg_sentence_length': 27.647058823529413, 'lexical_diversity': 0.45390070921985815, 'sentiment_scores': {'neg': 0.064, 'neu': 0.832, 'pos': 0.104, 'compound': 0.9942}, 'readability_fk_grade': 13.0, 'readability_fk_ease': 46.3, 'title': 'Impacts of COVID 19 on Streets Sides Food Stalls', 'filename': 'blackassign0087.txt'}\n",
            "{'word_count': 1382, 'sentence_count': 30, 'avg_sentence_length': 46.06666666666667, 'lexical_diversity': 0.46960926193921854, 'sentiment_scores': {'neg': 0.035, 'neu': 0.879, 'pos': 0.086, 'compound': 0.9957}, 'readability_fk_grade': 14.2, 'readability_fk_ease': 43.06, 'title': 'Rise of Electric Vehicles and its Impact on Livelihood by 2040', 'filename': 'blackassign0025.txt'}\n",
            "{'word_count': 2132, 'sentence_count': 89, 'avg_sentence_length': 23.95505617977528, 'lexical_diversity': 0.399155722326454, 'sentiment_scores': {'neg': 0.034, 'neu': 0.89, 'pos': 0.076, 'compound': 0.9936}, 'readability_fk_grade': 14.0, 'readability_fk_ease': 32.94, 'title': 'What Jobs Will Robots Take From Humans in The Future?', 'filename': 'blackassign0031.txt'}\n",
            "{'word_count': 2121, 'sentence_count': 65, 'avg_sentence_length': 32.63076923076923, 'lexical_diversity': 0.3785950023573786, 'sentiment_scores': {'neg': 0.071, 'neu': 0.862, 'pos': 0.067, 'compound': -0.088}, 'readability_fk_grade': 14.5, 'readability_fk_ease': 37.03, 'title': 'Coronavirus impact on energy markets', 'filename': 'blackassign0088.txt'}\n",
            "{'word_count': 637, 'sentence_count': 26, 'avg_sentence_length': 24.5, 'lexical_diversity': 0.5510204081632653, 'sentiment_scores': {'neg': 0.109, 'neu': 0.797, 'pos': 0.094, 'compound': -0.9483}, 'readability_fk_grade': 13.3, 'readability_fk_ease': 40.08, 'title': 'Will technology eliminate the need for animal testing in drug development?', 'filename': 'blackassign0063.txt'}\n",
            "{'word_count': 1035, 'sentence_count': 45, 'avg_sentence_length': 23.0, 'lexical_diversity': 0.47439613526570046, 'sentiment_scores': {'neg': 0.015, 'neu': 0.891, 'pos': 0.094, 'compound': 0.9962}, 'readability_fk_grade': 13.5, 'readability_fk_ease': 34.05, 'title': 'Evolution of Advertising Industry', 'filename': 'blackassign0047.txt'}\n",
            "{'word_count': 943, 'sentence_count': 41, 'avg_sentence_length': 23.0, 'lexical_diversity': 0.48250265111346763, 'sentiment_scores': {'neg': 0.006, 'neu': 0.824, 'pos': 0.169, 'compound': 0.9993}, 'readability_fk_grade': 13.1, 'readability_fk_ease': 35.17, 'title': 'How Python became the first choice for Data Science.', 'filename': 'blackassign0053.txt'}\n",
            "{'word_count': 2350, 'sentence_count': 114, 'avg_sentence_length': 20.614035087719298, 'lexical_diversity': 0.285531914893617, 'sentiment_scores': {'neg': 0.057, 'neu': 0.79, 'pos': 0.153, 'compound': 0.9997}, 'readability_fk_grade': 10.5, 'readability_fk_ease': 52.7, 'title': 'What is the difference between Artificial Intelligence, Machine Learning, Statistics, and Data Mining?', 'filename': 'blackassign0052.txt'}\n",
            "{'word_count': 2097, 'sentence_count': 13, 'avg_sentence_length': 161.30769230769232, 'lexical_diversity': 0.38912732474964234, 'sentiment_scores': {'neg': 0.059, 'neu': 0.809, 'pos': 0.132, 'compound': 0.9993}, 'readability_fk_grade': 67.4, 'readability_fk_ease': -95.3, 'title': 'How AI will impact the future of work?', 'filename': 'blackassign0045.txt'}\n",
            "{'word_count': 967, 'sentence_count': 34, 'avg_sentence_length': 28.441176470588236, 'lexical_diversity': 0.5025853154084798, 'sentiment_scores': {'neg': 0.037, 'neu': 0.871, 'pos': 0.092, 'compound': 0.9911}, 'readability_fk_grade': 13.4, 'readability_fk_ease': 39.87, 'title': 'How Small Business can survive the Coronavirus Crisis', 'filename': 'blackassign0076.txt'}\n",
            "{'word_count': 396, 'sentence_count': 14, 'avg_sentence_length': 28.285714285714285, 'lexical_diversity': 0.6515151515151515, 'sentiment_scores': {'neg': 0.018, 'neu': 0.886, 'pos': 0.095, 'compound': 0.9741}, 'readability_fk_grade': 17.4, 'readability_fk_ease': 13.07, 'title': 'How we forecast future technologies?', 'filename': 'blackassign0058.txt'}\n",
            "{'word_count': 1885, 'sentence_count': 85, 'avg_sentence_length': 22.176470588235293, 'lexical_diversity': 0.44562334217506633, 'sentiment_scores': {'neg': 0.047, 'neu': 0.844, 'pos': 0.109, 'compound': 0.9987}, 'readability_fk_grade': 11.2, 'readability_fk_ease': 50.97, 'title': 'What is the chance Homo sapiens will survive for the next 500 years?', 'filename': 'blackassign0066.txt'}\n",
            "{'word_count': 1218, 'sentence_count': 42, 'avg_sentence_length': 29.0, 'lexical_diversity': 0.45648604269293924, 'sentiment_scores': {'neg': 0.009, 'neu': 0.856, 'pos': 0.134, 'compound': 0.9991}, 'readability_fk_grade': 12.8, 'readability_fk_ease': 46.81, 'title': 'How Voice search makes your business a successful business.', 'filename': 'blackassign0084.txt'}\n",
            "{'word_count': 832, 'sentence_count': 32, 'avg_sentence_length': 26.0, 'lexical_diversity': 0.515625, 'sentiment_scores': {'neg': 0.03, 'neu': 0.842, 'pos': 0.128, 'compound': 0.9961}, 'readability_fk_grade': 11.6, 'readability_fk_ease': 49.86, 'title': 'How COVID-19 is impacting payment preferences?', 'filename': 'blackassign0099.txt'}\n",
            "{'word_count': 2273, 'sentence_count': 85, 'avg_sentence_length': 26.741176470588236, 'lexical_diversity': 0.39111306643202814, 'sentiment_scores': {'neg': 0.019, 'neu': 0.834, 'pos': 0.147, 'compound': 0.9998}, 'readability_fk_grade': 15.5, 'readability_fk_ease': 23.46, 'title': 'The rise of the OTT platform and its impact on the entertainment industry by 2040.', 'filename': 'blackassign0006.txt'}\n",
            "{'word_count': 1989, 'sentence_count': 96, 'avg_sentence_length': 20.71875, 'lexical_diversity': 0.38964303670186023, 'sentiment_scores': {'neg': 0.041, 'neu': 0.82, 'pos': 0.139, 'compound': 0.9995}, 'readability_fk_grade': 10.6, 'readability_fk_ease': 52.39, 'title': 'Will Machine Replace The Human in the Future of Work?', 'filename': 'blackassign0032.txt'}\n",
            "{'word_count': 1822, 'sentence_count': 80, 'avg_sentence_length': 22.775, 'lexical_diversity': 0.4154774972557629, 'sentiment_scores': {'neg': 0.034, 'neu': 0.821, 'pos': 0.145, 'compound': 0.9995}, 'readability_fk_grade': 13.0, 'readability_fk_ease': 35.37, 'title': 'Rising IT Cities and Their Impact on the Economy, Environment, Infrastructure, and City Life in Future', 'filename': 'blackassign0002.txt'}\n",
            "{'word_count': 2142, 'sentence_count': 99, 'avg_sentence_length': 21.636363636363637, 'lexical_diversity': 0.3716153127917834, 'sentiment_scores': {'neg': 0.011, 'neu': 0.859, 'pos': 0.13, 'compound': 0.9996}, 'readability_fk_grade': 11.9, 'readability_fk_ease': 43.73, 'title': 'How does marketing influence businesses and consumers?', 'filename': 'blackassign0019.txt'}\n",
            "{'word_count': 1347, 'sentence_count': 48, 'avg_sentence_length': 28.0625, 'lexical_diversity': 0.40014847809948034, 'sentiment_scores': {'neg': 0.091, 'neu': 0.777, 'pos': 0.132, 'compound': 0.9948}, 'readability_fk_grade': 13.0, 'readability_fk_ease': 46.2, 'title': 'Global financial crisis 2008 causes/effects and its solution', 'filename': 'blackassign0073.txt'}\n",
            "{'word_count': 424, 'sentence_count': 11, 'avg_sentence_length': 38.54545454545455, 'lexical_diversity': 0.6037735849056604, 'sentiment_scores': {'neg': 0.032, 'neu': 0.873, 'pos': 0.095, 'compound': 0.9648}, 'readability_fk_grade': 20.9, 'readability_fk_ease': 9.56, 'title': 'How Data Analytics can help your business respond to the impact of COVID-19?', 'filename': 'blackassign0048.txt'}\n",
            "{'word_count': 1106, 'sentence_count': 52, 'avg_sentence_length': 21.26923076923077, 'lexical_diversity': 0.46473779385171793, 'sentiment_scores': {'neg': 0.029, 'neu': 0.868, 'pos': 0.103, 'compound': 0.9968}, 'readability_fk_grade': 18.2, 'readability_fk_ease': 5.73, 'title': 'Rise of Internet Demand and Its Impact on Communications and Alternatives by the Year 2035', 'filename': 'blackassign0008.txt'}\n",
            "{'word_count': 1585, 'sentence_count': 63, 'avg_sentence_length': 25.158730158730158, 'lexical_diversity': 0.45678233438485805, 'sentiment_scores': {'neg': 0.051, 'neu': 0.757, 'pos': 0.192, 'compound': 0.9997}, 'readability_fk_grade': 12.9, 'readability_fk_ease': 41.09, 'title': 'Rise of telemedicine and its Impact on Livelihood by 2040', 'filename': 'blackassign0016.txt'}\n",
            "{'word_count': 1468, 'sentence_count': 65, 'avg_sentence_length': 22.584615384615386, 'lexical_diversity': 0.4298365122615804, 'sentiment_scores': {'neg': 0.054, 'neu': 0.822, 'pos': 0.125, 'compound': 0.9984}, 'readability_fk_grade': 13.7, 'readability_fk_ease': 33.54, 'title': 'Gender diversity and Equality in the tech industry', 'filename': 'blackassign0074.txt'}\n",
            "{'word_count': 1439, 'sentence_count': 61, 'avg_sentence_length': 23.59016393442623, 'lexical_diversity': 0.3891591382904795, 'sentiment_scores': {'neg': 0.046, 'neu': 0.838, 'pos': 0.117, 'compound': 0.998}, 'readability_fk_grade': 16.1, 'readability_fk_ease': 16.52, 'title': 'Internet Demand’s Evolution, Communication Impact, and 2035’s Alternative Pathways', 'filename': 'blackassign0003.txt'}\n",
            "{'word_count': 392, 'sentence_count': 9, 'avg_sentence_length': 43.55555555555556, 'lexical_diversity': 0.6454081632653061, 'sentiment_scores': {'neg': 0.064, 'neu': 0.852, 'pos': 0.084, 'compound': 0.6567}, 'readability_fk_grade': 15.7, 'readability_fk_ease': 28.37, 'title': 'Are we any closer to preventing a nuclear holocaust?', 'filename': 'blackassign0062.txt'}\n",
            "{'word_count': 1454, 'sentence_count': 66, 'avg_sentence_length': 22.03030303030303, 'lexical_diversity': 0.4518569463548831, 'sentiment_scores': {'neg': 0.018, 'neu': 0.865, 'pos': 0.118, 'compound': 0.9989}, 'readability_fk_grade': 13.1, 'readability_fk_ease': 35.27, 'title': 'Rise of Chatbots and its impact on customer support by the year 2040', 'filename': 'blackassign0017.txt'}\n",
            "{'word_count': 985, 'sentence_count': 43, 'avg_sentence_length': 22.906976744186046, 'lexical_diversity': 0.49441624365482234, 'sentiment_scores': {'neg': 0.006, 'neu': 0.891, 'pos': 0.103, 'compound': 0.9969}, 'readability_fk_grade': 13.5, 'readability_fk_ease': 34.15, 'title': 'OTT platform and its impact on the entertainment industry in Future.', 'filename': 'blackassign0005.txt'}\n",
            "{'word_count': 1516, 'sentence_count': 62, 'avg_sentence_length': 24.451612903225808, 'lexical_diversity': 0.44063324538258575, 'sentiment_scores': {'neg': 0.048, 'neu': 0.824, 'pos': 0.128, 'compound': 0.9989}, 'readability_fk_grade': 14.3, 'readability_fk_ease': 32.02, 'title': 'How Machines, AI, Automations, and Robo-human are Effective in Finance and Banking?', 'filename': 'blackassign0040.txt'}\n",
            "{'word_count': 1877, 'sentence_count': 64, 'avg_sentence_length': 29.328125, 'lexical_diversity': 0.41662226957911563, 'sentiment_scores': {'neg': 0.056, 'neu': 0.865, 'pos': 0.08, 'compound': 0.9871}, 'readability_fk_grade': 15.4, 'readability_fk_ease': 29.08, 'title': 'Online gaming: Adolescent online gaming effects demotivated, depression, musculoskeletal, and psychosomatic symptoms.', 'filename': 'blackassign0082.txt'}\n",
            "{'word_count': 424, 'sentence_count': 14, 'avg_sentence_length': 30.285714285714285, 'lexical_diversity': 0.6438679245283019, 'sentiment_scores': {'neg': 0.019, 'neu': 0.811, 'pos': 0.17, 'compound': 0.9954}, 'readability_fk_grade': 15.8, 'readability_fk_ease': 28.27, 'title': 'Embedding care robots into society and practice: Socio-technical considerations', 'filename': 'blackassign0060.txt'}\n",
            "{'word_count': 1197, 'sentence_count': 66, 'avg_sentence_length': 18.136363636363637, 'lexical_diversity': 0.4118629908103592, 'sentiment_scores': {'neg': 0.046, 'neu': 0.828, 'pos': 0.126, 'compound': 0.9984}, 'readability_fk_grade': 9.2, 'readability_fk_ease': 56.05, 'title': 'Why does your business need a chatbot?', 'filename': 'blackassign0067.txt'}\n",
            "{'word_count': 1023, 'sentence_count': 40, 'avg_sentence_length': 25.575, 'lexical_diversity': 0.46725317693059626, 'sentiment_scores': {'neg': 0.044, 'neu': 0.803, 'pos': 0.153, 'compound': 0.9988}, 'readability_fk_grade': 12.0, 'readability_fk_ease': 48.84, 'title': 'How machine learning will affect your business?', 'filename': 'blackassign0037.txt'}\n",
            "{'word_count': 1025, 'sentence_count': 33, 'avg_sentence_length': 31.060606060606062, 'lexical_diversity': 0.4926829268292683, 'sentiment_scores': {'neg': 0.02, 'neu': 0.905, 'pos': 0.075, 'compound': 0.9931}, 'readability_fk_grade': 14.8, 'readability_fk_ease': 36.02, 'title': 'How Data Analytics and AI are used to halt the COVID-19 Pandemic?', 'filename': 'blackassign0051.txt'}\n",
            "{'word_count': 1664, 'sentence_count': 55, 'avg_sentence_length': 30.254545454545454, 'lexical_diversity': 0.40805288461538464, 'sentiment_scores': {'neg': 0.06, 'neu': 0.832, 'pos': 0.109, 'compound': 0.9963}, 'readability_fk_grade': 13.2, 'readability_fk_ease': 45.69, 'title': 'Oil prices by the year 2040, and how it will impact the world economy.', 'filename': 'blackassign0027.txt'}\n",
            "{'word_count': 1263, 'sentence_count': 34, 'avg_sentence_length': 37.14705882352941, 'lexical_diversity': 0.4829770387965162, 'sentiment_scores': {'neg': 0.12, 'neu': 0.787, 'pos': 0.093, 'compound': -0.9936}, 'readability_fk_grade': 16.5, 'readability_fk_ease': 31.75, 'title': 'How will COVID-19 affect the world of work?', 'filename': 'blackassign0100.txt'}\n",
            "{'word_count': 1834, 'sentence_count': 67, 'avg_sentence_length': 27.37313432835821, 'lexical_diversity': 0.35605234460196294, 'sentiment_scores': {'neg': 0.02, 'neu': 0.857, 'pos': 0.123, 'compound': 0.9996}, 'readability_fk_grade': 11.6, 'readability_fk_ease': 55.17, 'title': 'How you lead a project or a team without any technical expertise?', 'filename': 'blackassign0068.txt'}\n",
            "{'word_count': 1472, 'sentence_count': 43, 'avg_sentence_length': 34.23255813953488, 'lexical_diversity': 0.44157608695652173, 'sentiment_scores': {'neg': 0.04, 'neu': 0.819, 'pos': 0.141, 'compound': 0.9993}, 'readability_fk_grade': 14.3, 'readability_fk_ease': 37.54, 'title': 'An outlook of healthcare by the year 2040, and how it will impact human lives.', 'filename': 'blackassign0028.txt'}\n",
            "{'word_count': 791, 'sentence_count': 26, 'avg_sentence_length': 30.423076923076923, 'lexical_diversity': 0.49810366624525915, 'sentiment_scores': {'neg': 0.007, 'neu': 0.881, 'pos': 0.112, 'compound': 0.9964}, 'readability_fk_grade': 16.4, 'readability_fk_ease': 26.54, 'title': 'How machine learning used in finance and banking?', 'filename': 'blackassign0044.txt'}\n",
            "{'word_count': 1134, 'sentence_count': 39, 'avg_sentence_length': 29.076923076923077, 'lexical_diversity': 0.4691358024691358, 'sentiment_scores': {'neg': 0.013, 'neu': 0.888, 'pos': 0.099, 'compound': 0.9973}, 'readability_fk_grade': 13.0, 'readability_fk_ease': 46.3, 'title': 'What is the future of mobile apps?', 'filename': 'blackassign0055.txt'}\n",
            "{'word_count': 1583, 'sentence_count': 59, 'avg_sentence_length': 26.83050847457627, 'lexical_diversity': 0.49715729627289956, 'sentiment_scores': {'neg': 0.113, 'neu': 0.8, 'pos': 0.087, 'compound': -0.992}, 'readability_fk_grade': 12.6, 'readability_fk_ease': 41.8, 'title': 'Estimating the impact of COVID-19 on the world of work', 'filename': 'blackassign0092.txt'}\n",
            "{'word_count': 2456, 'sentence_count': 94, 'avg_sentence_length': 26.127659574468087, 'lexical_diversity': 0.3513843648208469, 'sentiment_scores': {'neg': 0.091, 'neu': 0.794, 'pos': 0.116, 'compound': 0.9953}, 'readability_fk_grade': 12.4, 'readability_fk_ease': 42.41, 'title': 'How to protect future data and its privacy?', 'filename': 'blackassign0039.txt'}\n",
            "{'word_count': 554, 'sentence_count': 15, 'avg_sentence_length': 36.93333333333333, 'lexical_diversity': 0.5703971119133574, 'sentiment_scores': {'neg': 0.033, 'neu': 0.871, 'pos': 0.097, 'compound': 0.98}, 'readability_fk_grade': 19.1, 'readability_fk_ease': 19.64, 'title': 'Impacts of COVID 19 on Food products', 'filename': 'blackassign0077.txt'}\n",
            "{'word_count': 751, 'sentence_count': 20, 'avg_sentence_length': 37.55, 'lexical_diversity': 0.48468708388814913, 'sentiment_scores': {'neg': 0.028, 'neu': 0.827, 'pos': 0.145, 'compound': 0.9967}, 'readability_fk_grade': 17.2, 'readability_fk_ease': 29.93, 'title': 'Can You Be Great Leader Without Technical Expertise', 'filename': 'blackassign0069.txt'}\n",
            "{'word_count': 2223, 'sentence_count': 80, 'avg_sentence_length': 27.7875, 'lexical_diversity': 0.43499775078722447, 'sentiment_scores': {'neg': 0.041, 'neu': 0.817, 'pos': 0.141, 'compound': 0.9997}, 'readability_fk_grade': 15.0, 'readability_fk_ease': 30.3, 'title': 'AI in healthcare to Improve Patient Outcomes', 'filename': 'blackassign0029.txt'}\n",
            "{'word_count': 435, 'sentence_count': 14, 'avg_sentence_length': 31.071428571428573, 'lexical_diversity': 0.6344827586206897, 'sentiment_scores': {'neg': 0.012, 'neu': 0.883, 'pos': 0.105, 'compound': 0.9888}, 'readability_fk_grade': 16.8, 'readability_fk_ease': 25.63, 'title': 'Travel and Tourism Outlook', 'filename': 'blackassign0093.txt'}\n",
            "{'word_count': 2157, 'sentence_count': 81, 'avg_sentence_length': 26.62962962962963, 'lexical_diversity': 0.41770978210477516, 'sentiment_scores': {'neg': 0.045, 'neu': 0.812, 'pos': 0.143, 'compound': 0.9996}, 'readability_fk_grade': 12.9, 'readability_fk_ease': 41.09, 'title': 'Will AI Replace Us or Work With Us?', 'filename': 'blackassign0033.txt'}\n",
            "{'word_count': 2243, 'sentence_count': 124, 'avg_sentence_length': 18.088709677419356, 'lexical_diversity': 0.3464110566205974, 'sentiment_scores': {'neg': 0.045, 'neu': 0.861, 'pos': 0.094, 'compound': 0.9986}, 'readability_fk_grade': 9.6, 'readability_fk_ease': 55.03, 'title': 'How the COVID-19 crisis is redefining jobs and services?', 'filename': 'blackassign0085.txt'}\n",
            "{'word_count': 1862, 'sentence_count': 84, 'avg_sentence_length': 22.166666666666668, 'lexical_diversity': 0.38882921589688507, 'sentiment_scores': {'neg': 0.076, 'neu': 0.781, 'pos': 0.143, 'compound': 0.9987}, 'readability_fk_grade': 10.9, 'readability_fk_ease': 51.58, 'title': 'What if the Creation is Taking Over the Creator?', 'filename': 'blackassign0030.txt'}\n",
            "{'word_count': 757, 'sentence_count': 24, 'avg_sentence_length': 31.541666666666668, 'lexical_diversity': 0.5574636723910171, 'sentiment_scores': {'neg': 0.021, 'neu': 0.878, 'pos': 0.101, 'compound': 0.993}, 'readability_fk_grade': 16.5, 'readability_fk_ease': 26.24, 'title': 'Impact of AI in health and medicine', 'filename': 'blackassign0056.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas requests beautifulsoup4 openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7uEGoTrXPas",
        "outputId": "dec814c1-14c0-444d-c7fe-7cfb265bae26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Load the input data\n",
        "input_file_path = '/content/Input.xlsx'  # Update the path as necessary\n",
        "input_df = pd.read_excel(\"/content/Input.xlsx\")\n",
        "\n",
        "# Directory to save the articles\n",
        "output_dir = 'articles'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Extract the article title and text (customize this according to the website structure)\n",
        "            title = soup.find('h1').get_text(strip=True)\n",
        "            paragraphs = soup.find_all('p')\n",
        "            article_text = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "            return title, article_text\n",
        "        else:\n",
        "            print(f\"Failed to retrieve the article: {url}\")\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while extracting {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Loop through the URLs and save the extracted articles\n",
        "for index, row in input_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    title, article_text = extract_article_text(url)\n",
        "\n",
        "    if title and article_text:\n",
        "        with open(os.path.join(output_dir, f\"{url_id}.txt\"), 'w', encoding='utf-8') as file:\n",
        "            file.write(f\"{title}\\n\\n{article_text}\")\n",
        "\n",
        "print(\"Article extraction completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dZ7REr7XWcu",
        "outputId": "2f4d81cd-1b80-4b2c-a271-93723a928766"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve the article: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Failed to retrieve the article: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Article extraction completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to compute text analysis variables\n",
        "def analyze_text(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    word_count = len(words)\n",
        "    sentence_count = text.count('.') + text.count('!') + text.count('?')\n",
        "    average_word_length = sum(len(word) for word in words) / word_count if word_count else 0\n",
        "\n",
        "    # Add more variables as required\n",
        "\n",
        "    return {\n",
        "        'Word Count': word_count,\n",
        "        'Sentence Count': sentence_count,\n",
        "        'Average Word Length': average_word_length,\n",
        "        # Add more variables here\n",
        "    }\n",
        "\n",
        "# Directory where articles are saved\n",
        "articles_dir = 'articles'\n",
        "\n",
        "# List to store analysis results\n",
        "results = []\n",
        "\n",
        "# Loop through the saved articles and analyze each one\n",
        "for article_file in os.listdir(articles_dir):\n",
        "    if article_file.endswith('.txt'):\n",
        "        with open(os.path.join(articles_dir, article_file), 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            analysis = analyze_text(text)\n",
        "            analysis['URL_ID'] = article_file.replace('.txt', '')\n",
        "            results.append(analysis)\n",
        "\n",
        "# Create a DataFrame and save to Excel\n",
        "output_df = pd.DataFrame(results)\n",
        "output_df.to_excel('Output Data Structure.xlsx', index=False)\n",
        "\n",
        "print(\"Text analysis completed and saved to Output Data Structure.xlsx.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9dXyAVhXmTy",
        "outputId": "8f91f4ca-bb66-4e09-f3f6-834bc7dff306"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text analysis completed and saved to Output Data Structure.xlsx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Load the structure of the output data\n",
        "output_structure_path = '/mnt/data/Output Data Structure.xlsx'\n",
        "output_structure_df = pd.read_excel(\"/content/Output Data Structure.xlsx\")\n",
        "output_columns = output_structure_df.columns.tolist()\n",
        "\n",
        "# Directory where articles are saved\n",
        "articles_dir = '/mnt/data/articles'\n",
        "\n",
        "# Function to compute text analysis variables\n",
        "def analyze_text(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    word_count = len(words)\n",
        "    sentence_count = text.count('.') + text.count('!') + text.count('?')\n",
        "    average_word_length = sum(len(word) for word in words) / word_count if word_count else 0\n",
        "\n",
        "    # Dummy values for other required columns (to be implemented)\n",
        "    variables = {\n",
        "        'Word Count': word_count,\n",
        "        'Sentence Count': sentence_count,\n",
        "        'Average Word Length': average_word_length,\n",
        "        # Add other variables here\n",
        "    }\n",
        "\n",
        "    return variables\n",
        "\n",
        "# List to store analysis results\n",
        "results = []\n",
        "\n",
        "# Loop through the saved articles and analyze each one\n",
        "for article_file in os.listdir(articles_dir):\n",
        "    if article_file.endswith('.txt'):\n",
        "        with open(os.path.join(articles_dir, article_file), 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            analysis = analyze_text(text)\n",
        "            analysis['URL_ID'] = article_file.replace('.txt', '')\n",
        "            results.append(analysis)\n",
        "\n",
        "# Create a DataFrame with the required structure and save to Excel\n",
        "output_df = pd.DataFrame(results, columns=['URL_ID'] + output_columns)\n",
        "output_df.to_excel('/mnt/data/Output Data Structure Completed.xlsx', index=False)\n",
        "\n",
        "print(\"Text analysis completed and saved to Output Data Structure Completed.xlsx.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V79qwrmqY5AB",
        "outputId": "c955d878-1ae6-4823-a642-bb3fca897759"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text analysis completed and saved to Output Data Structure Completed.xlsx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Load the structure of the output data\n",
        "output_structure_path = 'Output Data Structure.xlsx'  # Update the path as necessary\n",
        "output_structure_df = pd.read_excel(\"/content/Output Data Structure.xlsx\")\n",
        "output_columns = output_structure_df.columns.tolist()\n",
        "\n",
        "# Directory where articles are saved\n",
        "articles_dir = 'articles'\n",
        "\n",
        "# Function to compute text analysis variables\n",
        "def analyze_text(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    word_count = len(words)\n",
        "    sentence_count = text.count('.') + text.count('!') + text.count('?')\n",
        "    average_word_length = sum(len(word) for word in words) / word_count if word_count else 0\n",
        "\n",
        "    # Dummy values for other required columns (to be implemented)\n",
        "    variables = {\n",
        "        'Word Count': word_count,\n",
        "        'Sentence Count': sentence_count,\n",
        "        'Average Word Length': average_word_length,\n",
        "        # Add other variables here\n",
        "    }\n",
        "\n",
        "    return variables\n",
        "\n",
        "# List to store analysis results\n",
        "results = []\n",
        "\n",
        "# Loop through the saved articles and analyze each one\n",
        "for article_file in os.listdir(articles_dir):\n",
        "    if article_file.endswith('.txt'):\n",
        "        with open(os.path.join(articles_dir, article_file), 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            analysis = analyze_text(text)\n",
        "            analysis['URL_ID'] = article_file.replace('.txt', '')\n",
        "            results.append(analysis)\n",
        "\n",
        "# Create a DataFrame with the required structure and save to Excel\n",
        "output_df = pd.DataFrame(results, columns=['URL_ID'] + output_columns)\n",
        "output_df.to_excel('Output Data Structure Completed.xlsx', index=False)\n",
        "\n",
        "print(\"Text analysis completed and saved to Output Data Structure Completed.xlsx.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XKovlaSdF5c",
        "outputId": "b42c9e02-d936-4946-d929-ff2dbe2a5479"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text analysis completed and saved to Output Data Structure Completed.xlsx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yivhUj5ow9rA",
        "outputId": "d5c45581-65ca-4a8f-f0e1-03f2ae3636a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Load the structure of the output data\n",
        "output_structure_path = '/mnt/data/Output Data Structure.xlsx'\n",
        "output_structure_df = pd.read_excel(\"/content/Output Data Structure.xlsx\")\n",
        "output_columns = output_structure_df.columns.tolist()\n",
        "\n",
        "# Directory where articles are saved\n",
        "articles_dir = '/mnt/data/articles'\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to calculate syllables\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "    vowels = \"aeiou\"\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith(\"e\"):\n",
        "        count -= 1\n",
        "    if word.endswith(\"le\") and len(word) > 2 and word[-3] not in vowels:\n",
        "        count += 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "# Load positive and negative words dictionaries\n",
        "with open('/content/positive-words.txt', 'r') as f:\n",
        "    positive_words = set(f.read().split())\n",
        "with open('/content/negative-words.txt', 'r') as f:\n",
        "    negative_words = set(f.read().split())\n",
        "\n",
        "# Function to clean text and calculate various metrics\n",
        "def analyze_text(text):\n",
        "    # Clean text\n",
        "    words = word_tokenize(text)\n",
        "    words_cleaned = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Calculate word count\n",
        "    word_count = len(words_cleaned)\n",
        "\n",
        "    # Calculate sentence count\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    # Calculate positive and negative scores\n",
        "    positive_score = sum(1 for word in words_cleaned if word.lower() in positive_words)\n",
        "    negative_score = sum(1 for word in words_cleaned if word.lower() in negative_words)\n",
        "\n",
        "    # Calculate polarity score\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "\n",
        "    # Calculate subjectivity score\n",
        "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
        "\n",
        "    # Calculate average sentence length\n",
        "    average_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "\n",
        "    # Calculate complex word count and percentage of complex words\n",
        "    complex_words = [word for word in words_cleaned if count_syllables(word) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "    percentage_complex_words = complex_word_count / word_count if word_count else 0\n",
        "\n",
        "    # Calculate fog index\n",
        "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "\n",
        "    # Calculate syllables per word\n",
        "    syllables_per_word = sum(count_syllables(word) for word in words_cleaned) / word_count if word_count else 0\n",
        "\n",
        "    # Calculate personal pronouns\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "    # Calculate average word length\n",
        "    average_word_length = sum(len(word) for word in words_cleaned) / word_count if word_count else 0\n",
        "\n",
        "    return {\n",
        "        'POSITIVE SCORE': positive_score,\n",
        "        'NEGATIVE SCORE': negative_score,\n",
        "        'POLARITY SCORE': polarity_score,\n",
        "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "        'AVG SENTENCE LENGTH': average_sentence_length,\n",
        "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "        'FOG INDEX': fog_index,\n",
        "        'AVG NUMBER OF WORDS PER SENTENCE': average_sentence_length,\n",
        "        'COMPLEX WORD COUNT': complex_word_count,\n",
        "        'WORD COUNT': word_count,\n",
        "        'SYLLABLE PER WORD': syllables_per_word,\n",
        "        'PERSONAL PRONOUNS': personal_pronouns,\n",
        "        'AVG WORD LENGTH': average_word_length\n",
        "    }\n",
        "\n",
        "# List to store analysis results\n",
        "results = []\n",
        "\n",
        "# Loop through the saved articles and analyze each one\n",
        "for article_file in os.listdir(articles_dir):\n",
        "    if article_file.endswith('.txt'):\n",
        "        with open(os.path.join(articles_dir, article_file), 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            analysis = analyze_text(text)\n",
        "            analysis['URL_ID'] = article_file.replace('.txt', '')\n",
        "            results.append(analysis)\n",
        "\n",
        "# Create a DataFrame with the required structure and save to Excel\n",
        "output_df = pd.DataFrame(results, columns=['URL_ID'] + output_columns)\n",
        "output_df.to_excel('/mnt/data/Output Data Structure Completed.xlsx', index=False)\n",
        "\n",
        "print(\"Text analysis completed and saved to Output Data Structure Completed.xlsx.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oyTgK5UIgUyX",
        "outputId": "3575af8d-a629-4eed-cc5b-f90395aa5ad9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b48c698dae20>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Function to calculate syllables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the structure of the output data\n",
        "output_structure_path = '/mnt/data/Output Data Structure.xlsx'\n",
        "output_structure_df = pd.read_excel(\"/content/Output Data Structure.xlsx\")\n",
        "output_columns = output_structure_df.columns.tolist()\n",
        "\n",
        "# Directory where articles are saved\n",
        "articles_dir = '/mnt/data/articles'\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to calculate syllables\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "    vowels = \"aeiou\"\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith(\"e\"):\n",
        "        count -= 1\n",
        "    if word.endswith(\"le\") and len(word) > 2 and word[-3] not in vowels:\n",
        "        count += 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "# Load positive and negative words dictionaries\n",
        "positive_words_path = '/mnt/data/positive-words.txt'\n",
        "negative_words_path = '/mnt/data/negative-words.txt'\n",
        "\n",
        "with open('/content/positive-words.txt', 'r', encoding='latin-1') as f:\n",
        "    positive_words = set(f.read().split())\n",
        "with open('/content/negative-words.txt', 'r', encoding='latin-1') as f:\n",
        "    negative_words = set(f.read().split())\n",
        "\n",
        "# Function to clean text and calculate various metrics\n",
        "def analyze_text(text):\n",
        "    # Clean text\n",
        "    words = word_tokenize(text)\n",
        "    words_cleaned = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Calculate word count\n",
        "    word_count = len(words_cleaned)\n",
        "\n",
        "    # Calculate sentence count\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    # Calculate positive and negative scores\n",
        "    positive_score = sum(1 for word in words_cleaned if word.lower() in positive_words)\n",
        "    negative_score = sum(1 for word in words_cleaned if word.lower() in negative_words)\n",
        "\n",
        "    # Calculate polarity score\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "\n",
        "    # Calculate subjectivity score\n",
        "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
        "\n",
        "    # Calculate average sentence length\n",
        "    average_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "\n",
        "    # Calculate complex word count and percentage of complex words\n",
        "    complex_words = [word for word in words_cleaned if count_syllables(word) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "    percentage_complex_words = complex_word_count / word_count if word_count else 0\n",
        "\n",
        "    # Calculate fog index\n",
        "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "\n",
        "    # Calculate syllables per word\n",
        "    syllables_per_word = sum(count_syllables(word) for word in words_cleaned) / word_count if word_count else 0\n",
        "\n",
        "    # Calculate personal pronouns\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "    # Calculate average word length\n",
        "    average_word_length = sum(len(word) for word in words_cleaned) / word_count if word_count else 0\n",
        "\n",
        "    return {\n",
        "        'POSITIVE SCORE': positive_score,\n",
        "        'NEGATIVE SCORE': negative_score,\n",
        "        'POLARITY SCORE': polarity_score,\n",
        "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "        'AVG SENTENCE LENGTH': average_sentence_length,\n",
        "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "        'FOG INDEX': fog_index,\n",
        "        'AVG NUMBER OF WORDS PER SENTENCE': average_sentence_length,\n",
        "        'COMPLEX WORD COUNT': complex_word_count,\n",
        "        'WORD COUNT': word_count,\n",
        "        'SYLLABLE PER WORD': syllables_per_word,\n",
        "        'PERSONAL PRONOUNS': personal_pronouns,\n",
        "        'AVG WORD LENGTH': average_word_length\n",
        "    }\n",
        "\n",
        "# List to store analysis results\n",
        "results = []\n",
        "\n",
        "# Loop through the saved articles and analyze each one\n",
        "for article_file in os.listdir(articles_dir):\n",
        "    if article_file.endswith('.txt'):\n",
        "        with open(os.path.join(articles_dir, article_file), 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            text = file.read()\n",
        "            analysis = analyze_text(text)\n",
        "            analysis['URL_ID'] = article_file.replace('.txt', '')\n",
        "            results.append(analysis)\n",
        "\n",
        "# Create a DataFrame with the required structure and save to Excel\n",
        "output_df = pd.DataFrame(results, columns=['URL_ID'] + output_columns)\n",
        "output_df.to_excel('/mnt/data/Output Data Structure Completed.xlsx', index=False)\n",
        "\n",
        "print(\"Text analysis completed and saved to Output Data Structure Completed.xlsx.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-796x5gNh1hv",
        "outputId": "4a28fec3-385d-4127-acdc-830ae9f5cd3a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text analysis completed and saved to Output Data Structure Completed.xlsx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the input data\n",
        "input_path = '/mnt/data/Input.xlsx'\n",
        "input_df = pd.read_excel(\"/content/Input.xlsx\")\n",
        "\n",
        "# Load the structure of the output data\n",
        "output_structure_path = '/mnt/data/Output Data Structure.xlsx'\n",
        "output_structure_df = pd.read_excel('/content/Output Data Structure.xlsx')\n",
        "output_columns = output_structure_df.columns.tolist()\n",
        "\n",
        "# Directory where articles are saved\n",
        "articles_dir = '/mnt/data/articles'\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to calculate syllables\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "    vowels = \"aeiou\"\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith(\"e\"):\n",
        "        count -= 1\n",
        "    if word.endswith(\"le\") and len(word) > 2 and word[-3] not in vowels:\n",
        "        count += 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "# Load positive and negative words dictionaries\n",
        "positive_words_path = '/mnt/data/positive-words.txt'\n",
        "negative_words_path = '/mnt/data/negative-words.txt'\n",
        "\n",
        "with open('/content/positive-words.txt', 'r', encoding='latin-1') as f:\n",
        "    positive_words = set(f.read().split())\n",
        "with open('/content/negative-words.txt', 'r', encoding='latin-1') as f:\n",
        "    negative_words = set(f.read().split())\n",
        "\n",
        "# Function to clean text and calculate various metrics\n",
        "def analyze_text(text):\n",
        "    # Clean text\n",
        "    words = word_tokenize(text)\n",
        "    words_cleaned = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Calculate word count\n",
        "    word_count = len(words_cleaned)\n",
        "\n",
        "    # Calculate sentence count\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    # Calculate positive and negative scores\n",
        "    positive_score = sum(1 for word in words_cleaned if word.lower() in positive_words)\n",
        "    negative_score = sum(1 for word in words_cleaned if word.lower() in negative_words)\n",
        "\n",
        "    # Calculate polarity score\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "\n",
        "    # Calculate subjectivity score\n",
        "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
        "\n",
        "    # Calculate average sentence length\n",
        "    average_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "\n",
        "    # Calculate complex word count and percentage of complex words\n",
        "    complex_words = [word for word in words_cleaned if count_syllables(word) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "    percentage_complex_words = complex_word_count / word_count if word_count else 0\n",
        "\n",
        "    # Calculate fog index\n",
        "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "\n",
        "    # Calculate syllables per word\n",
        "    syllables_per_word = sum(count_syllables(word) for word in words_cleaned) / word_count if word_count else 0\n",
        "\n",
        "    # Calculate personal pronouns\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "    # Calculate average word length\n",
        "    average_word_length = sum(len(word) for word in words_cleaned) / word_count if word_count else 0\n",
        "\n",
        "    return {\n",
        "        'POSITIVE SCORE': positive_score,\n",
        "        'NEGATIVE SCORE': negative_score,\n",
        "        'POLARITY SCORE': polarity_score,\n",
        "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "        'AVG SENTENCE LENGTH': average_sentence_length,\n",
        "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "        'FOG INDEX': fog_index,\n",
        "        'AVG NUMBER OF WORDS PER SENTENCE': average_sentence_length,\n",
        "        'COMPLEX WORD COUNT': complex_word_count,\n",
        "        'WORD COUNT': word_count,\n",
        "        'SYLLABLE PER WORD': syllables_per_word,\n",
        "        'PERSONAL PRONOUNS': personal_pronouns,\n",
        "        'AVG WORD LENGTH': average_word_length\n",
        "    }\n",
        "\n",
        "# List to store analysis results\n",
        "results = []\n",
        "\n",
        "# Loop through the saved articles and analyze each one\n",
        "for article_file in os.listdir(articles_dir):\n",
        "    if article_file.endswith('.txt'):\n",
        "        with open(os.path.join(articles_dir, article_file), 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            text = file.read()\n",
        "            analysis = analyze_text(text)\n",
        "            analysis['URL_ID'] = article_file.replace('.txt', '')\n",
        "            results.append(analysis)\n",
        "\n",
        "# Convert analysis results to DataFrame\n",
        "analysis_df = pd.DataFrame(results)\n",
        "\n",
        "# Merge the input data with the analysis results\n",
        "merged_df = input_df.merge(analysis_df, on='URL_ID')\n",
        "\n",
        "# Ensure the columns are in the correct order as specified in the output structure\n",
        "output_df = merged_df[output_columns]\n",
        "\n",
        "# Save the final DataFrame to Excel\n",
        "output_df.to_excel('/mnt/data/Output Data Structure Completed.xlsx', index=False)\n",
        "\n",
        "print(\"Text analysis completed and saved to Output Data Structure Completed.xlsx.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "slBy8Vhskejb",
        "outputId": "63bfd150-0bba-4638-f935-eb7548ad8b23"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Word Count', 'Sentence Count', 'Average Word Length'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6b36b76bbcb6>\u001b[0m in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# Ensure the columns are in the correct order as specified in the output structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0moutput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;31m# Save the final DataFrame to Excel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3766\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3767\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3769\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5875\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5877\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5879\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5940\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5943\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Word Count', 'Sentence Count', 'Average Word Length'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pwvcl-0hy6zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the input data\n",
        "input_path = '/mnt/data/Input.xlsx'\n",
        "input_df = pd.read_excel(\"/content/Input.xlsx\")\n",
        "\n",
        "# Load the structure of the output data\n",
        "output_structure_path = '/mnt/data/Output Data Structure.xlsx'\n",
        "output_structure_df = pd.read_excel('/content/Output Data Structure.xlsx')\n",
        "output_columns = output_structure_df.columns.tolist()\n",
        "\n",
        "# Directory where articles are saved\n",
        "articles_dir = '/mnt/data/articles'\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to calculate syllables\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "    vowels = \"aeiou\"\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith(\"e\"):\n",
        "        count -= 1\n",
        "    if word.endswith(\"le\") and len(word) > 2 and word[-3] not in vowels:\n",
        "        count += 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "# Load positive and negative words dictionaries\n",
        "positive_words_path = '/mnt/data/positive-words.txt'\n",
        "negative_words_path = '/mnt/data/negative-words.txt'\n",
        "\n",
        "with open('/content/positive-words.txt', 'r', encoding='latin-1') as f:\n",
        "    positive_words = set(f.read().split())\n",
        "with open('/content/negative-words.txt', 'r', encoding='latin-1') as f:\n",
        "    negative_words = set(f.read().split())\n",
        "\n",
        "# Function to clean text and calculate various metrics\n",
        "def analyze_text(text):\n",
        "    # Clean text\n",
        "    words = word_tokenize(text)\n",
        "    words_cleaned = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Calculate word count\n",
        "    word_count = len(words_cleaned)\n",
        "\n",
        "    # Calculate sentence count\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    # Calculate positive and negative scores\n",
        "    positive_score = sum(1 for word in words_cleaned if word.lower() in positive_words)\n",
        "    negative_score = sum(1 for word in words_cleaned if word.lower() in negative_words)\n",
        "\n",
        "    # Calculate polarity score\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "\n",
        "    # Calculate subjectivity score\n",
        "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
        "\n",
        "    # Calculate average sentence length\n",
        "    average_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "\n",
        "    # Calculate complex word count and percentage of complex words\n",
        "    complex_words = [word for word in words_cleaned if count_syllables(word) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "    percentage_complex_words = complex_word_count / word_count if word_count else 0\n",
        "\n",
        "    # Calculate fog index\n",
        "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "\n",
        "    # Calculate syllables per word\n",
        "    syllables_per_word = sum(count_syllables(word) for word in words_cleaned) / word_count if word_count else 0\n",
        "\n",
        "    # Calculate personal pronouns\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "    # Calculate average word length\n",
        "    average_word_length = sum(len(word) for word in words_cleaned) / word_count if word_count else 0\n",
        "\n",
        "    return {\n",
        "        'POSITIVE SCORE': positive_score,\n",
        "        'NEGATIVE SCORE': negative_score,\n",
        "        'POLARITY SCORE': polarity_score,\n",
        "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "        'AVG SENTENCE LENGTH': average_sentence_length,\n",
        "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "        'FOG INDEX': fog_index,\n",
        "        'AVG NUMBER OF WORDS PER SENTENCE': average_sentence_length,\n",
        "        'COMPLEX WORD COUNT': complex_word_count,\n",
        "        'WORD COUNT': word_count,\n",
        "        'SYLLABLE PER WORD': syllables_per_word,\n",
        "        'PERSONAL PRONOUNS': personal_pronouns,\n",
        "        'AVG WORD LENGTH': average_word_length\n",
        "    }\n",
        "\n",
        "# List to store analysis results\n",
        "results = []\n",
        "\n",
        "# Loop through the saved articles and analyze each one\n",
        "for article_file in os.listdir(articles_dir):\n",
        "    if article_file.endswith('.txt'):\n",
        "        with open(os.path.join(articles_dir, article_file), 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            text = file.read()\n",
        "            analysis = analyze_text(text)\n",
        "            analysis['URL_ID'] = article_file.replace('.txt', '')\n",
        "            results.append(analysis)\n",
        "\n",
        "# Convert analysis results to DataFrame\n",
        "analysis_df = pd.DataFrame(results)\n",
        "\n",
        "# Merge the input data with the analysis results\n",
        "merged_df = input_df.merge(analysis_df, on='URL_ID')\n",
        "\n",
        "# Print the columns of merged_df\n",
        "print(\"Columns in merged_df:\")\n",
        "print(merged_df.columns)\n",
        "\n",
        "output_df = merged_df[output_columns]\n",
        "\n",
        "output_df.to_excel('/mnt/data/Output Data Structure Completed.xlsx', index=False)\n",
        "\n",
        "print(\"Text analysis completed and saved to Output Data Structure Completed.xlsx.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "yUAuL3YM0MUl",
        "outputId": "6952850e-3e6b-4aa6-acba-25bfbf036364"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in merged_df:\n",
            "Index(['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
            "       'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
            "       'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
            "       'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
            "       'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Word Count', 'Sentence Count', 'Average Word Length'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5c053510a8b8>\u001b[0m in \u001b[0;36m<cell line: 134>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0moutput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0moutput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/data/Output Data Structure Completed.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3766\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3767\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3769\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5875\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5877\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5879\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5940\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5943\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Word Count', 'Sentence Count', 'Average Word Length'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGm8Lc6z07u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the input data\n",
        "input_path = '/mnt/data/Input.xlsx'\n",
        "input_df = pd.read_excel(\"/content/Input.xlsx\")\n",
        "\n",
        "# Load the structure of the output data\n",
        "output_structure_path = '/mnt/data/Output Data Structure.xlsx'\n",
        "output_structure_df = pd.read_excel('/content/Output Data Structure.xlsx')\n",
        "output_columns = ['URL_ID'] + [\n",
        "    'URL',\n",
        "    'POSITIVE SCORE',\n",
        "    'NEGATIVE SCORE',\n",
        "    'POLARITY SCORE',\n",
        "    'SUBJECTIVITY SCORE',\n",
        "    'AVG SENTENCE LENGTH',\n",
        "    'PERCENTAGE OF COMPLEX WORDS',\n",
        "    'FOG INDEX',\n",
        "    'AVG NUMBER OF WORDS PER SENTENCE',\n",
        "    'COMPLEX WORD COUNT',\n",
        "    'WORD COUNT',\n",
        "    'SYLLABLE PER WORD',\n",
        "    'PERSONAL PRONOUNS',\n",
        "    'AVG WORD LENGTH'\n",
        "]\n",
        "# Directory where articles are saved\n",
        "articles_dir = '/mnt/data/articles'\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to calculate syllables\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "    vowels = \"aeiou\"\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith(\"e\"):\n",
        "        count -= 1\n",
        "    if word.endswith(\"le\") and len(word) > 2 and word[-3] not in vowels:\n",
        "        count += 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "# Load positive and negative words dictionaries\n",
        "positive_words_path = '/mnt/data/positive-words.txt'\n",
        "negative_words_path = '/mnt/data/negative-words.txt'\n",
        "\n",
        "with open('/content/positive-words.txt', 'r', encoding='latin-1') as f:\n",
        "    positive_words = set(f.read().split())\n",
        "with open('/content/negative-words.txt', 'r', encoding='latin-1') as f:\n",
        "    negative_words = set(f.read().split())\n",
        "\n",
        "# Function to clean text and calculate various metrics\n",
        "def analyze_text(text):\n",
        "    # Clean text\n",
        "    words = word_tokenize(text)\n",
        "    words_cleaned = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Calculate word count\n",
        "    word_count = len(words_cleaned)\n",
        "\n",
        "    # Calculate sentence count\n",
        "    sentence_count = len(sentences)\n",
        "\n",
        "    # Calculate positive and negative scores\n",
        "    positive_score = sum(1 for word in words_cleaned if word.lower() in positive_words)\n",
        "    negative_score = sum(1 for word in words_cleaned if word.lower() in negative_words)\n",
        "\n",
        "    # Calculate polarity score\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "\n",
        "    # Calculate subjectivity score\n",
        "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
        "\n",
        "    # Calculate average sentence length\n",
        "    average_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "\n",
        "    # Calculate complex word count and percentage of complex words\n",
        "    complex_words = [word for word in words_cleaned if count_syllables(word) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "    percentage_complex_words = complex_word_count / word_count if word_count else 0\n",
        "\n",
        "    # Calculate fog index\n",
        "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "\n",
        "    # Calculate syllables per word\n",
        "    syllables_per_word = sum(count_syllables(word) for word in words_cleaned) / word_count if word_count else 0\n",
        "\n",
        "    # Calculate personal pronouns\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
        "\n",
        "    # Calculate average word length\n",
        "    average_word_length = sum(len(word) for word in words_cleaned) / word_count if word_count else 0\n",
        "\n",
        "    return {\n",
        "        'POSITIVE SCORE': positive_score,\n",
        "        'NEGATIVE SCORE': negative_score,\n",
        "        'POLARITY SCORE': polarity_score,\n",
        "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "        'AVG SENTENCE LENGTH': average_sentence_length,\n",
        "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "        'FOG INDEX': fog_index,\n",
        "        'AVG NUMBER OF WORDS PER SENTENCE': average_sentence_length,\n",
        "        'COMPLEX WORD COUNT': complex_word_count,\n",
        "        'WORD COUNT': word_count,\n",
        "        'SYLLABLE PER WORD': syllables_per_word,\n",
        "        'PERSONAL PRONOUNS': personal_pronouns,\n",
        "        'AVG WORD LENGTH': average_word_length\n",
        "    }\n",
        "\n",
        "# List to store analysis results\n",
        "results = []\n",
        "\n",
        "# Loop through the saved articles and analyze each one\n",
        "for article_file in os.listdir(articles_dir):\n",
        "    if article_file.endswith('.txt'):\n",
        "        with open(os.path.join(articles_dir, article_file), 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            text = file.read()\n",
        "            analysis = analyze_text(text)\n",
        "            analysis['URL_ID'] = article_file.replace('.txt', '')\n",
        "            results.append(analysis)\n",
        "\n",
        "# Convert analysis results to DataFrame\n",
        "analysis_df = pd.DataFrame(results)\n",
        "\n",
        "# Merge the input data with the analysis results\n",
        "merged_df = input_df.merge(analysis_df, on='URL_ID')\n",
        "\n",
        "# Print the columns of merged_df\n",
        "print(\"Columns in merged_df:\")\n",
        "print(merged_df.columns)\n",
        "\n",
        "output_df = merged_df[output_columns]\n",
        "\n",
        "output_df.to_excel('/mnt/data/Output Data Structure Completed.xlsx', index=False)\n",
        "\n",
        "print(\"Text analysis completed and saved to Output Data Structure Completed.xlsx.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-22Jow-1BtY",
        "outputId": "082ece42-75e2-4057-971a-6d15b59c2955"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in merged_df:\n",
            "Index(['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
            "       'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
            "       'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
            "       'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
            "       'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'],\n",
            "      dtype='object')\n",
            "Text analysis completed and saved to Output Data Structure Completed.xlsx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pipreqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3gxP0hyi1-Ir",
        "outputId": "0cabfe55-8733-4baf-9c9a-f32164d5f4d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pipreqs\n",
            "  Downloading pipreqs-0.5.0-py3-none-any.whl (33 kB)\n",
            "Collecting docopt==0.6.2 (from pipreqs)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ipython==8.12.3 (from pipreqs)\n",
            "  Downloading ipython-8.12.3-py3-none-any.whl (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.3/798.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nbconvert<8.0.0,>=7.11.0 (from pipreqs)\n",
            "  Downloading nbconvert-7.16.4-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.4/257.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarg==0.1.9 (from pipreqs)\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (4.4.2)\n",
            "Collecting jedi>=0.16 (from ipython==8.12.3->pipreqs)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (0.1.7)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (3.0.47)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (2.16.1)\n",
            "Collecting stack-data (from ipython==8.12.3->pipreqs)\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (5.7.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (4.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from yarg==0.1.9->pipreqs) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (2.1.5)\n",
            "Collecting mistune<4,>=2.0.3 (from nbconvert<8.0.0,>=7.11.0->pipreqs)\n",
            "  Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.10.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (1.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==8.12.3->pipreqs) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.2.2)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (6.1.12)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.19.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==8.12.3->pipreqs) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython==8.12.3->pipreqs) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->yarg==0.1.9->pipreqs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->yarg==0.1.9->pipreqs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->yarg==0.1.9->pipreqs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->yarg==0.1.9->pipreqs) (2024.6.2)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython==8.12.3->pipreqs)\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython==8.12.3->pipreqs)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting pure-eval (from stack-data->ipython==8.12.3->pipreqs)\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.18.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (6.3.3)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=33fdc1eca727a41df4723a67c812b847728e3111ab74703e868dc1d91af34904\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pure-eval, docopt, mistune, jedi, executing, asttokens, yarg, stack-data, ipython, nbconvert, pipreqs\n",
            "  Attempting uninstall: mistune\n",
            "    Found existing installation: mistune 0.8.4\n",
            "    Uninstalling mistune-0.8.4:\n",
            "      Successfully uninstalled mistune-0.8.4\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.34.0\n",
            "    Uninstalling ipython-7.34.0:\n",
            "      Successfully uninstalled ipython-7.34.0\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 6.5.4\n",
            "    Uninstalling nbconvert-6.5.4:\n",
            "      Successfully uninstalled nbconvert-6.5.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asttokens-2.4.1 docopt-0.6.2 executing-2.0.1 ipython-8.12.3 jedi-0.19.1 mistune-3.0.2 nbconvert-7.16.4 pipreqs-0.5.0 pure-eval-0.2.2 stack-data-0.6.3 yarg-0.1.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython"
                ]
              },
              "id": "9bf12bfcb0464e638716efd98405ab72"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhxaVoP-3JDH",
        "outputId": "4aed290d-25d7-4e41-dbfb-ebbd40d49621"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                          Version\n",
            "-------------------------------- ---------------------\n",
            "absl-py                          1.4.0\n",
            "aiohttp                          3.9.5\n",
            "aiosignal                        1.3.1\n",
            "alabaster                        0.7.16\n",
            "albumentations                   1.3.1\n",
            "altair                           4.2.2\n",
            "annotated-types                  0.7.0\n",
            "anyio                            3.7.1\n",
            "argon2-cffi                      23.1.0\n",
            "argon2-cffi-bindings             21.2.0\n",
            "array_record                     0.5.1\n",
            "arviz                            0.15.1\n",
            "astropy                          5.3.4\n",
            "asttokens                        2.4.1\n",
            "astunparse                       1.6.3\n",
            "async-timeout                    4.0.3\n",
            "atpublic                         4.1.0\n",
            "attrs                            23.2.0\n",
            "audioread                        3.0.1\n",
            "autograd                         1.6.2\n",
            "Babel                            2.15.0\n",
            "backcall                         0.2.0\n",
            "beautifulsoup4                   4.12.3\n",
            "bidict                           0.23.1\n",
            "bigframes                        1.8.0\n",
            "bleach                           6.1.0\n",
            "blinker                          1.4\n",
            "blis                             0.7.11\n",
            "blosc2                           2.0.0\n",
            "bokeh                            3.3.4\n",
            "bqplot                           0.12.43\n",
            "branca                           0.7.2\n",
            "build                            1.2.1\n",
            "CacheControl                     0.14.0\n",
            "cachetools                       5.3.3\n",
            "catalogue                        2.0.10\n",
            "certifi                          2024.6.2\n",
            "cffi                             1.16.0\n",
            "chardet                          5.2.0\n",
            "charset-normalizer               3.3.2\n",
            "chex                             0.1.86\n",
            "click                            8.1.7\n",
            "click-plugins                    1.1.1\n",
            "cligj                            0.7.2\n",
            "cloudpathlib                     0.18.1\n",
            "cloudpickle                      2.2.1\n",
            "cmake                            3.27.9\n",
            "cmdstanpy                        1.2.4\n",
            "colorcet                         3.1.0\n",
            "colorlover                       0.3.0\n",
            "colour                           0.1.5\n",
            "community                        1.0.0b1\n",
            "confection                       0.1.5\n",
            "cons                             0.4.6\n",
            "contextlib2                      21.6.0\n",
            "contourpy                        1.2.1\n",
            "cryptography                     42.0.8\n",
            "cuda-python                      12.2.1\n",
            "cudf-cu12                        24.4.1\n",
            "cufflinks                        0.17.3\n",
            "cupy-cuda12x                     12.2.0\n",
            "cvxopt                           1.3.2\n",
            "cvxpy                            1.3.4\n",
            "cycler                           0.12.1\n",
            "cymem                            2.0.8\n",
            "Cython                           3.0.10\n",
            "dask                             2023.8.1\n",
            "datascience                      0.17.6\n",
            "db-dtypes                        1.2.0\n",
            "dbus-python                      1.2.18\n",
            "debugpy                          1.6.6\n",
            "decorator                        4.4.2\n",
            "defusedxml                       0.7.1\n",
            "distributed                      2023.8.1\n",
            "distro                           1.7.0\n",
            "dlib                             19.24.4\n",
            "dm-tree                          0.1.8\n",
            "docopt                           0.6.2\n",
            "docstring_parser                 0.16\n",
            "docutils                         0.18.1\n",
            "dopamine_rl                      4.0.9\n",
            "duckdb                           0.10.3\n",
            "earthengine-api                  0.1.407\n",
            "easydict                         1.13\n",
            "ecos                             2.0.14\n",
            "editdistance                     0.6.2\n",
            "eerepr                           0.0.4\n",
            "en-core-web-sm                   3.7.1\n",
            "entrypoints                      0.4\n",
            "et-xmlfile                       1.1.0\n",
            "etils                            1.7.0\n",
            "etuples                          0.3.9\n",
            "exceptiongroup                   1.2.1\n",
            "executing                        2.0.1\n",
            "fastai                           2.7.15\n",
            "fastcore                         1.5.46\n",
            "fastdownload                     0.0.7\n",
            "fastjsonschema                   2.20.0\n",
            "fastprogress                     1.0.3\n",
            "fastrlock                        0.8.2\n",
            "filelock                         3.15.1\n",
            "fiona                            1.9.6\n",
            "firebase-admin                   5.3.0\n",
            "Flask                            2.2.5\n",
            "flatbuffers                      24.3.25\n",
            "flax                             0.8.4\n",
            "folium                           0.14.0\n",
            "fonttools                        4.53.0\n",
            "frozendict                       2.4.4\n",
            "frozenlist                       1.4.1\n",
            "fsspec                           2023.6.0\n",
            "future                           0.18.3\n",
            "gast                             0.5.4\n",
            "gcsfs                            2023.6.0\n",
            "GDAL                             3.6.4\n",
            "gdown                            5.1.0\n",
            "geemap                           0.32.1\n",
            "gensim                           4.3.2\n",
            "geocoder                         1.38.1\n",
            "geographiclib                    2.0\n",
            "geopandas                        0.13.2\n",
            "geopy                            2.3.0\n",
            "gin-config                       0.5.0\n",
            "glob2                            0.7\n",
            "google                           2.0.3\n",
            "google-ai-generativelanguage     0.6.4\n",
            "google-api-core                  2.11.1\n",
            "google-api-python-client         2.84.0\n",
            "google-auth                      2.27.0\n",
            "google-auth-httplib2             0.1.1\n",
            "google-auth-oauthlib             1.2.0\n",
            "google-cloud-aiplatform          1.56.0\n",
            "google-cloud-bigquery            3.21.0\n",
            "google-cloud-bigquery-connection 1.12.1\n",
            "google-cloud-bigquery-storage    2.25.0\n",
            "google-cloud-core                2.3.3\n",
            "google-cloud-datastore           2.15.2\n",
            "google-cloud-firestore           2.11.1\n",
            "google-cloud-functions           1.13.3\n",
            "google-cloud-iam                 2.15.0\n",
            "google-cloud-language            2.13.3\n",
            "google-cloud-resource-manager    1.12.3\n",
            "google-cloud-storage             2.8.0\n",
            "google-cloud-translate           3.11.3\n",
            "google-colab                     1.0.0\n",
            "google-crc32c                    1.5.0\n",
            "google-generativeai              0.5.4\n",
            "google-pasta                     0.2.0\n",
            "google-resumable-media           2.7.1\n",
            "googleapis-common-protos         1.63.1\n",
            "googledrivedownloader            0.4\n",
            "graphviz                         0.20.3\n",
            "greenlet                         3.0.3\n",
            "grpc-google-iam-v1               0.13.0\n",
            "grpcio                           1.64.1\n",
            "grpcio-status                    1.48.2\n",
            "gspread                          6.0.2\n",
            "gspread-dataframe                3.3.1\n",
            "gym                              0.25.2\n",
            "gym-notices                      0.0.8\n",
            "h5netcdf                         1.3.0\n",
            "h5py                             3.9.0\n",
            "holidays                         0.51\n",
            "holoviews                        1.17.1\n",
            "html5lib                         1.1\n",
            "httpimport                       1.3.1\n",
            "httplib2                         0.22.0\n",
            "huggingface-hub                  0.23.4\n",
            "humanize                         4.7.0\n",
            "hyperopt                         0.2.7\n",
            "ibis-framework                   8.0.0\n",
            "idna                             3.7\n",
            "imageio                          2.31.6\n",
            "imageio-ffmpeg                   0.5.1\n",
            "imagesize                        1.4.1\n",
            "imbalanced-learn                 0.10.1\n",
            "imgaug                           0.4.0\n",
            "immutabledict                    4.2.0\n",
            "importlib_metadata               7.1.0\n",
            "importlib_resources              6.4.0\n",
            "imutils                          0.5.4\n",
            "inflect                          7.0.0\n",
            "iniconfig                        2.0.0\n",
            "intel-openmp                     2023.2.4\n",
            "ipyevents                        2.0.2\n",
            "ipyfilechooser                   0.6.0\n",
            "ipykernel                        5.5.6\n",
            "ipyleaflet                       0.18.2\n",
            "ipython                          8.12.3\n",
            "ipython-genutils                 0.2.0\n",
            "ipython-sql                      0.5.0\n",
            "ipytree                          0.2.2\n",
            "ipywidgets                       7.7.1\n",
            "itsdangerous                     2.2.0\n",
            "jax                              0.4.26\n",
            "jaxlib                           0.4.26+cuda12.cudnn89\n",
            "jedi                             0.19.1\n",
            "jeepney                          0.7.1\n",
            "jellyfish                        1.0.4\n",
            "jieba                            0.42.1\n",
            "Jinja2                           3.1.4\n",
            "joblib                           1.4.2\n",
            "jsonpickle                       3.2.1\n",
            "jsonschema                       4.19.2\n",
            "jsonschema-specifications        2023.12.1\n",
            "jupyter-client                   6.1.12\n",
            "jupyter-console                  6.1.0\n",
            "jupyter_core                     5.7.2\n",
            "jupyter-server                   1.24.0\n",
            "jupyterlab_pygments              0.3.0\n",
            "jupyterlab_widgets               3.0.11\n",
            "kaggle                           1.6.14\n",
            "kagglehub                        0.2.5\n",
            "keras                            2.15.0\n",
            "keyring                          23.5.0\n",
            "kiwisolver                       1.4.5\n",
            "langcodes                        3.4.0\n",
            "language_data                    1.2.0\n",
            "launchpadlib                     1.10.16\n",
            "lazr.restfulclient               0.14.4\n",
            "lazr.uri                         1.0.6\n",
            "lazy_loader                      0.4\n",
            "libclang                         18.1.1\n",
            "librosa                          0.10.2.post1\n",
            "lightgbm                         4.1.0\n",
            "linkify-it-py                    2.0.3\n",
            "llvmlite                         0.41.1\n",
            "locket                           1.0.0\n",
            "logical-unification              0.4.6\n",
            "lxml                             4.9.4\n",
            "malloy                           2023.1067\n",
            "marisa-trie                      1.2.0\n",
            "Markdown                         3.6\n",
            "markdown-it-py                   3.0.0\n",
            "MarkupSafe                       2.1.5\n",
            "matplotlib                       3.7.1\n",
            "matplotlib-inline                0.1.7\n",
            "matplotlib-venn                  0.11.10\n",
            "mdit-py-plugins                  0.4.1\n",
            "mdurl                            0.1.2\n",
            "miniKanren                       1.0.3\n",
            "missingno                        0.5.2\n",
            "mistune                          3.0.2\n",
            "mizani                           0.9.3\n",
            "mkl                              2023.2.0\n",
            "ml-dtypes                        0.2.0\n",
            "mlxtend                          0.22.0\n",
            "more-itertools                   10.1.0\n",
            "moviepy                          1.0.3\n",
            "mpmath                           1.3.0\n",
            "msgpack                          1.0.8\n",
            "multidict                        6.0.5\n",
            "multipledispatch                 1.0.0\n",
            "multitasking                     0.0.11\n",
            "murmurhash                       1.0.10\n",
            "music21                          9.1.0\n",
            "natsort                          8.4.0\n",
            "nbclassic                        1.1.0\n",
            "nbclient                         0.10.0\n",
            "nbconvert                        7.16.4\n",
            "nbformat                         5.10.4\n",
            "nest-asyncio                     1.6.0\n",
            "networkx                         3.3\n",
            "nibabel                          4.0.2\n",
            "nltk                             3.8.1\n",
            "notebook                         6.5.5\n",
            "notebook_shim                    0.2.4\n",
            "numba                            0.58.1\n",
            "numexpr                          2.10.0\n",
            "numpy                            1.25.2\n",
            "nvtx                             0.2.10\n",
            "oauth2client                     4.1.3\n",
            "oauthlib                         3.2.2\n",
            "opencv-contrib-python            4.8.0.76\n",
            "opencv-python                    4.8.0.76\n",
            "opencv-python-headless           4.10.0.84\n",
            "openpyxl                         3.1.4\n",
            "opt-einsum                       3.3.0\n",
            "optax                            0.2.2\n",
            "orbax-checkpoint                 0.4.4\n",
            "osqp                             0.6.2.post8\n",
            "packaging                        24.1\n",
            "pandas                           2.0.3\n",
            "pandas-datareader                0.10.0\n",
            "pandas-gbq                       0.19.2\n",
            "pandas-stubs                     2.0.3.230814\n",
            "pandocfilters                    1.5.1\n",
            "panel                            1.3.8\n",
            "param                            2.1.0\n",
            "parso                            0.8.4\n",
            "parsy                            2.1\n",
            "partd                            1.4.2\n",
            "pathlib                          1.0.1\n",
            "patsy                            0.5.6\n",
            "peewee                           3.17.5\n",
            "pexpect                          4.9.0\n",
            "pickleshare                      0.7.5\n",
            "Pillow                           9.4.0\n",
            "pip                              23.1.2\n",
            "pip-tools                        6.13.0\n",
            "pipreqs                          0.5.0\n",
            "platformdirs                     4.2.2\n",
            "plotly                           5.15.0\n",
            "plotnine                         0.12.4\n",
            "pluggy                           1.5.0\n",
            "polars                           0.20.2\n",
            "pooch                            1.8.2\n",
            "portpicker                       1.5.2\n",
            "prefetch-generator               1.0.3\n",
            "preshed                          3.0.9\n",
            "prettytable                      3.10.0\n",
            "proglog                          0.1.10\n",
            "progressbar2                     4.2.0\n",
            "prometheus_client                0.20.0\n",
            "promise                          2.3\n",
            "prompt_toolkit                   3.0.47\n",
            "prophet                          1.1.5\n",
            "proto-plus                       1.23.0\n",
            "protobuf                         3.20.3\n",
            "psutil                           5.9.5\n",
            "psycopg2                         2.9.9\n",
            "ptyprocess                       0.7.0\n",
            "pure-eval                        0.2.2\n",
            "py-cpuinfo                       9.0.0\n",
            "py4j                             0.10.9.7\n",
            "pyarrow                          14.0.2\n",
            "pyarrow-hotfix                   0.6\n",
            "pyasn1                           0.6.0\n",
            "pyasn1_modules                   0.4.0\n",
            "pycocotools                      2.0.8\n",
            "pycparser                        2.22\n",
            "pydantic                         2.7.4\n",
            "pydantic_core                    2.18.4\n",
            "pydata-google-auth               1.8.2\n",
            "pydot                            1.4.2\n",
            "pydot-ng                         2.0.0\n",
            "pydotplus                        2.0.2\n",
            "PyDrive                          1.3.1\n",
            "PyDrive2                         1.6.3\n",
            "pyerfa                           2.0.1.4\n",
            "pygame                           2.5.2\n",
            "Pygments                         2.16.1\n",
            "PyGObject                        3.42.1\n",
            "PyJWT                            2.3.0\n",
            "pymc                             5.10.4\n",
            "pymystem3                        0.2.0\n",
            "pynvjitlink-cu12                 0.2.4\n",
            "PyOpenGL                         3.1.7\n",
            "pyOpenSSL                        24.1.0\n",
            "pyparsing                        3.1.2\n",
            "pyperclip                        1.8.2\n",
            "pyphen                           0.15.0\n",
            "pyproj                           3.6.1\n",
            "pyproject_hooks                  1.1.0\n",
            "pyshp                            2.3.1\n",
            "PySocks                          1.7.1\n",
            "pytensor                         2.18.6\n",
            "pytest                           7.4.4\n",
            "python-apt                       0.0.0\n",
            "python-box                       7.2.0\n",
            "python-dateutil                  2.8.2\n",
            "python-louvain                   0.16\n",
            "python-slugify                   8.0.4\n",
            "python-utils                     3.8.2\n",
            "pytz                             2023.4\n",
            "pyviz_comms                      3.0.2\n",
            "PyWavelets                       1.6.0\n",
            "PyYAML                           6.0.1\n",
            "pyzmq                            24.0.1\n",
            "qdldl                            0.1.7.post3\n",
            "qudida                           0.0.4\n",
            "ratelim                          0.1.6\n",
            "referencing                      0.35.1\n",
            "regex                            2024.5.15\n",
            "requests                         2.31.0\n",
            "requests-oauthlib                1.3.1\n",
            "requirements-parser              0.9.0\n",
            "rich                             13.7.1\n",
            "rmm-cu12                         24.4.0\n",
            "rpds-py                          0.18.1\n",
            "rpy2                             3.4.2\n",
            "rsa                              4.9\n",
            "safetensors                      0.4.3\n",
            "scikit-image                     0.19.3\n",
            "scikit-learn                     1.2.2\n",
            "scipy                            1.11.4\n",
            "scooby                           0.10.0\n",
            "scs                              3.2.4.post2\n",
            "seaborn                          0.13.1\n",
            "SecretStorage                    3.3.1\n",
            "Send2Trash                       1.8.3\n",
            "sentencepiece                    0.1.99\n",
            "setuptools                       67.7.2\n",
            "shapely                          2.0.4\n",
            "shellingham                      1.5.4\n",
            "simple_parsing                   0.1.5\n",
            "six                              1.16.0\n",
            "sklearn-pandas                   2.2.0\n",
            "smart-open                       7.0.4\n",
            "sniffio                          1.3.1\n",
            "snowballstemmer                  2.2.0\n",
            "sortedcontainers                 2.4.0\n",
            "soundfile                        0.12.1\n",
            "soupsieve                        2.5\n",
            "soxr                             0.3.7\n",
            "spacy                            3.7.5\n",
            "spacy-legacy                     3.0.12\n",
            "spacy-loggers                    1.0.5\n",
            "Sphinx                           5.0.2\n",
            "sphinxcontrib-applehelp          1.0.8\n",
            "sphinxcontrib-devhelp            1.0.6\n",
            "sphinxcontrib-htmlhelp           2.0.5\n",
            "sphinxcontrib-jsmath             1.0.1\n",
            "sphinxcontrib-qthelp             1.0.7\n",
            "sphinxcontrib-serializinghtml    1.1.10\n",
            "SQLAlchemy                       2.0.30\n",
            "sqlglot                          20.11.0\n",
            "sqlparse                         0.5.0\n",
            "srsly                            2.4.8\n",
            "stack-data                       0.6.3\n",
            "stanio                           0.5.0\n",
            "statsmodels                      0.14.2\n",
            "StrEnum                          0.4.15\n",
            "sympy                            1.12.1\n",
            "tables                           3.8.0\n",
            "tabulate                         0.9.0\n",
            "tbb                              2021.12.0\n",
            "tblib                            3.0.0\n",
            "tenacity                         8.4.1\n",
            "tensorboard                      2.15.2\n",
            "tensorboard-data-server          0.7.2\n",
            "tensorflow                       2.15.0\n",
            "tensorflow-datasets              4.9.6\n",
            "tensorflow-estimator             2.15.0\n",
            "tensorflow-gcs-config            2.15.0\n",
            "tensorflow-hub                   0.16.1\n",
            "tensorflow-io-gcs-filesystem     0.37.0\n",
            "tensorflow-metadata              1.15.0\n",
            "tensorflow-probability           0.23.0\n",
            "tensorstore                      0.1.45\n",
            "termcolor                        2.4.0\n",
            "terminado                        0.18.1\n",
            "text-unidecode                   1.3\n",
            "textblob                         0.17.1\n",
            "textstat                         0.7.3\n",
            "tf_keras                         2.15.1\n",
            "tf-slim                          1.1.0\n",
            "thinc                            8.2.4\n",
            "threadpoolctl                    3.5.0\n",
            "tifffile                         2024.5.22\n",
            "tinycss2                         1.3.0\n",
            "tokenizers                       0.19.1\n",
            "toml                             0.10.2\n",
            "tomli                            2.0.1\n",
            "toolz                            0.12.1\n",
            "torch                            2.3.0+cu121\n",
            "torchaudio                       2.3.0+cu121\n",
            "torchsummary                     1.5.1\n",
            "torchtext                        0.18.0\n",
            "torchvision                      0.18.0+cu121\n",
            "tornado                          6.3.3\n",
            "tqdm                             4.66.4\n",
            "traitlets                        5.7.1\n",
            "traittypes                       0.2.1\n",
            "transformers                     4.41.2\n",
            "triton                           2.3.0\n",
            "tweepy                           4.14.0\n",
            "typer                            0.12.3\n",
            "types-pytz                       2024.1.0.20240417\n",
            "types-setuptools                 70.0.0.20240524\n",
            "typing_extensions                4.12.2\n",
            "tzdata                           2024.1\n",
            "tzlocal                          5.2\n",
            "uc-micro-py                      1.0.3\n",
            "uritemplate                      4.1.1\n",
            "urllib3                          2.0.7\n",
            "vega-datasets                    0.9.0\n",
            "wadllib                          1.3.6\n",
            "wasabi                           1.1.3\n",
            "wcwidth                          0.2.13\n",
            "weasel                           0.4.1\n",
            "webcolors                        24.6.0\n",
            "webencodings                     0.5.1\n",
            "websocket-client                 1.8.0\n",
            "Werkzeug                         3.0.3\n",
            "wheel                            0.43.0\n",
            "widgetsnbextension               3.6.6\n",
            "wordcloud                        1.9.3\n",
            "wrapt                            1.14.1\n",
            "xarray                           2023.7.0\n",
            "xarray-einstats                  0.7.0\n",
            "xgboost                          2.0.3\n",
            "xlrd                             2.0.1\n",
            "xyzservices                      2024.6.0\n",
            "yarg                             0.1.9\n",
            "yarl                             1.9.4\n",
            "yellowbrick                      1.5\n",
            "yfinance                         0.2.40\n",
            "zict                             3.0.0\n",
            "zipp                             3.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list --format=freeze"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhLxSIiD3Uq6",
        "outputId": "00b23080-2188-4b9e-92bc-cb685fa018cc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "aiohttp==3.9.5\n",
            "aiosignal==1.3.1\n",
            "alabaster==0.7.16\n",
            "albumentations==1.3.1\n",
            "altair==4.2.2\n",
            "annotated-types==0.7.0\n",
            "anyio==3.7.1\n",
            "argon2-cffi==23.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array_record==0.5.1\n",
            "arviz==0.15.1\n",
            "astropy==5.3.4\n",
            "asttokens==2.4.1\n",
            "astunparse==1.6.3\n",
            "async-timeout==4.0.3\n",
            "atpublic==4.1.0\n",
            "attrs==23.2.0\n",
            "audioread==3.0.1\n",
            "autograd==1.6.2\n",
            "Babel==2.15.0\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.12.3\n",
            "bidict==0.23.1\n",
            "bigframes==1.8.0\n",
            "bleach==6.1.0\n",
            "blinker==1.4\n",
            "blis==0.7.11\n",
            "blosc2==2.0.0\n",
            "bokeh==3.3.4\n",
            "bqplot==0.12.43\n",
            "branca==0.7.2\n",
            "build==1.2.1\n",
            "CacheControl==0.14.0\n",
            "cachetools==5.3.3\n",
            "catalogue==2.0.10\n",
            "certifi==2024.6.2\n",
            "cffi==1.16.0\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.3.2\n",
            "chex==0.1.86\n",
            "click==8.1.7\n",
            "click-plugins==1.1.1\n",
            "cligj==0.7.2\n",
            "cloudpathlib==0.18.1\n",
            "cloudpickle==2.2.1\n",
            "cmake==3.27.9\n",
            "cmdstanpy==1.2.4\n",
            "colorcet==3.1.0\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.5\n",
            "cons==0.4.6\n",
            "contextlib2==21.6.0\n",
            "contourpy==1.2.1\n",
            "cryptography==42.0.8\n",
            "cuda-python==12.2.1\n",
            "cudf-cu12==24.4.1\n",
            "cufflinks==0.17.3\n",
            "cupy-cuda12x==12.2.0\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.3.4\n",
            "cycler==0.12.1\n",
            "cymem==2.0.8\n",
            "Cython==3.0.10\n",
            "dask==2023.8.1\n",
            "datascience==0.17.6\n",
            "db-dtypes==1.2.0\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.6.6\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "distributed==2023.8.1\n",
            "distro==1.7.0\n",
            "dlib==19.24.4\n",
            "dm-tree==0.1.8\n",
            "docopt==0.6.2\n",
            "docstring_parser==0.16\n",
            "docutils==0.18.1\n",
            "dopamine_rl==4.0.9\n",
            "duckdb==0.10.3\n",
            "earthengine-api==0.1.407\n",
            "easydict==1.13\n",
            "ecos==2.0.14\n",
            "editdistance==0.6.2\n",
            "eerepr==0.0.4\n",
            "en-core-web-sm==3.7.1\n",
            "entrypoints==0.4\n",
            "et-xmlfile==1.1.0\n",
            "etils==1.7.0\n",
            "etuples==0.3.9\n",
            "exceptiongroup==1.2.1\n",
            "executing==2.0.1\n",
            "fastai==2.7.15\n",
            "fastcore==1.5.46\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.20.0\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.2\n",
            "filelock==3.15.1\n",
            "fiona==1.9.6\n",
            "firebase-admin==5.3.0\n",
            "Flask==2.2.5\n",
            "flatbuffers==24.3.25\n",
            "flax==0.8.4\n",
            "folium==0.14.0\n",
            "fonttools==4.53.0\n",
            "frozendict==2.4.4\n",
            "frozenlist==1.4.1\n",
            "fsspec==2023.6.0\n",
            "future==0.18.3\n",
            "gast==0.5.4\n",
            "gcsfs==2023.6.0\n",
            "GDAL==3.6.4\n",
            "gdown==5.1.0\n",
            "geemap==0.32.1\n",
            "gensim==4.3.2\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==0.13.2\n",
            "geopy==2.3.0\n",
            "gin-config==0.5.0\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-ai-generativelanguage==0.6.4\n",
            "google-api-core==2.11.1\n",
            "google-api-python-client==2.84.0\n",
            "google-auth==2.27.0\n",
            "google-auth-httplib2==0.1.1\n",
            "google-auth-oauthlib==1.2.0\n",
            "google-cloud-aiplatform==1.56.0\n",
            "google-cloud-bigquery==3.21.0\n",
            "google-cloud-bigquery-connection==1.12.1\n",
            "google-cloud-bigquery-storage==2.25.0\n",
            "google-cloud-core==2.3.3\n",
            "google-cloud-datastore==2.15.2\n",
            "google-cloud-firestore==2.11.1\n",
            "google-cloud-functions==1.13.3\n",
            "google-cloud-iam==2.15.0\n",
            "google-cloud-language==2.13.3\n",
            "google-cloud-resource-manager==1.12.3\n",
            "google-cloud-storage==2.8.0\n",
            "google-cloud-translate==3.11.3\n",
            "google-colab==1.0.0\n",
            "google-crc32c==1.5.0\n",
            "google-generativeai==0.5.4\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.7.1\n",
            "googleapis-common-protos==1.63.1\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.20.3\n",
            "greenlet==3.0.3\n",
            "grpc-google-iam-v1==0.13.0\n",
            "grpcio==1.64.1\n",
            "grpcio-status==1.48.2\n",
            "gspread==6.0.2\n",
            "gspread-dataframe==3.3.1\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h5netcdf==1.3.0\n",
            "h5py==3.9.0\n",
            "holidays==0.51\n",
            "holoviews==1.17.1\n",
            "html5lib==1.1\n",
            "httpimport==1.3.1\n",
            "httplib2==0.22.0\n",
            "huggingface-hub==0.23.4\n",
            "humanize==4.7.0\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==8.0.0\n",
            "idna==3.7\n",
            "imageio==2.31.6\n",
            "imageio-ffmpeg==0.5.1\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.10.1\n",
            "imgaug==0.4.0\n",
            "immutabledict==4.2.0\n",
            "importlib_metadata==7.1.0\n",
            "importlib_resources==6.4.0\n",
            "imutils==0.5.4\n",
            "inflect==7.0.0\n",
            "iniconfig==2.0.0\n",
            "intel-openmp==2023.2.4\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==5.5.6\n",
            "ipyleaflet==0.18.2\n",
            "ipython==8.12.3\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.2.0\n",
            "jax==0.4.26\n",
            "jaxlib==0.4.26+cuda12.cudnn89\n",
            "jedi==0.19.1\n",
            "jeepney==0.7.1\n",
            "jellyfish==1.0.4\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.4\n",
            "joblib==1.4.2\n",
            "jsonpickle==3.2.1\n",
            "jsonschema==4.19.2\n",
            "jsonschema-specifications==2023.12.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter_core==5.7.2\n",
            "jupyter-server==1.24.0\n",
            "jupyterlab_pygments==0.3.0\n",
            "jupyterlab_widgets==3.0.11\n",
            "kaggle==1.6.14\n",
            "kagglehub==0.2.5\n",
            "keras==2.15.0\n",
            "keyring==23.5.0\n",
            "kiwisolver==1.4.5\n",
            "langcodes==3.4.0\n",
            "language_data==1.2.0\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.4\n",
            "libclang==18.1.1\n",
            "librosa==0.10.2.post1\n",
            "lightgbm==4.1.0\n",
            "linkify-it-py==2.0.3\n",
            "llvmlite==0.41.1\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "lxml==4.9.4\n",
            "malloy==2023.1067\n",
            "marisa-trie==1.2.0\n",
            "Markdown==3.6\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==2.1.5\n",
            "matplotlib==3.7.1\n",
            "matplotlib-inline==0.1.7\n",
            "matplotlib-venn==0.11.10\n",
            "mdit-py-plugins==0.4.1\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==3.0.2\n",
            "mizani==0.9.3\n",
            "mkl==2023.2.0\n",
            "ml-dtypes==0.2.0\n",
            "mlxtend==0.22.0\n",
            "more-itertools==10.1.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.0.8\n",
            "multidict==6.0.5\n",
            "multipledispatch==1.0.0\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.10\n",
            "music21==9.1.0\n",
            "natsort==8.4.0\n",
            "nbclassic==1.1.0\n",
            "nbclient==0.10.0\n",
            "nbconvert==7.16.4\n",
            "nbformat==5.10.4\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.3\n",
            "nibabel==4.0.2\n",
            "nltk==3.8.1\n",
            "notebook==6.5.5\n",
            "notebook_shim==0.2.4\n",
            "numba==0.58.1\n",
            "numexpr==2.10.0\n",
            "numpy==1.25.2\n",
            "nvtx==0.2.10\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "opencv-contrib-python==4.8.0.76\n",
            "opencv-python==4.8.0.76\n",
            "opencv-python-headless==4.10.0.84\n",
            "openpyxl==3.1.4\n",
            "opt-einsum==3.3.0\n",
            "optax==0.2.2\n",
            "orbax-checkpoint==0.4.4\n",
            "osqp==0.6.2.post8\n",
            "packaging==24.1\n",
            "pandas==2.0.3\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.19.2\n",
            "pandas-stubs==2.0.3.230814\n",
            "pandocfilters==1.5.1\n",
            "panel==1.3.8\n",
            "param==2.1.0\n",
            "parso==0.8.4\n",
            "parsy==2.1\n",
            "partd==1.4.2\n",
            "pathlib==1.0.1\n",
            "patsy==0.5.6\n",
            "peewee==3.17.5\n",
            "pexpect==4.9.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==9.4.0\n",
            "pip==23.1.2\n",
            "pip-tools==6.13.0\n",
            "pipreqs==0.5.0\n",
            "platformdirs==4.2.2\n",
            "plotly==5.15.0\n",
            "plotnine==0.12.4\n",
            "pluggy==1.5.0\n",
            "polars==0.20.2\n",
            "pooch==1.8.2\n",
            "portpicker==1.5.2\n",
            "prefetch-generator==1.0.3\n",
            "preshed==3.0.9\n",
            "prettytable==3.10.0\n",
            "proglog==0.1.10\n",
            "progressbar2==4.2.0\n",
            "prometheus_client==0.20.0\n",
            "promise==2.3\n",
            "prompt_toolkit==3.0.47\n",
            "prophet==1.1.5\n",
            "proto-plus==1.23.0\n",
            "protobuf==3.20.3\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.9\n",
            "ptyprocess==0.7.0\n",
            "pure-eval==0.2.2\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==14.0.2\n",
            "pyarrow-hotfix==0.6\n",
            "pyasn1==0.6.0\n",
            "pyasn1_modules==0.4.0\n",
            "pycocotools==2.0.8\n",
            "pycparser==2.22\n",
            "pydantic==2.7.4\n",
            "pydantic_core==2.18.4\n",
            "pydata-google-auth==1.8.2\n",
            "pydot==1.4.2\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.6.3\n",
            "pyerfa==2.0.1.4\n",
            "pygame==2.5.2\n",
            "Pygments==2.16.1\n",
            "PyGObject==3.42.1\n",
            "PyJWT==2.3.0\n",
            "pymc==5.10.4\n",
            "pymystem3==0.2.0\n",
            "pynvjitlink-cu12==0.2.4\n",
            "PyOpenGL==3.1.7\n",
            "pyOpenSSL==24.1.0\n",
            "pyparsing==3.1.2\n",
            "pyperclip==1.8.2\n",
            "pyphen==0.15.0\n",
            "pyproj==3.6.1\n",
            "pyproject_hooks==1.1.0\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pytensor==2.18.6\n",
            "pytest==7.4.4\n",
            "python-apt==0.0.0\n",
            "python-box==7.2.0\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.4\n",
            "python-utils==3.8.2\n",
            "pytz==2023.4\n",
            "pyviz_comms==3.0.2\n",
            "PyWavelets==1.6.0\n",
            "PyYAML==6.0.1\n",
            "pyzmq==24.0.1\n",
            "qdldl==0.1.7.post3\n",
            "qudida==0.0.4\n",
            "ratelim==0.1.6\n",
            "referencing==0.35.1\n",
            "regex==2024.5.15\n",
            "requests==2.31.0\n",
            "requests-oauthlib==1.3.1\n",
            "requirements-parser==0.9.0\n",
            "rich==13.7.1\n",
            "rmm-cu12==24.4.0\n",
            "rpds-py==0.18.1\n",
            "rpy2==3.4.2\n",
            "rsa==4.9\n",
            "safetensors==0.4.3\n",
            "scikit-image==0.19.3\n",
            "scikit-learn==1.2.2\n",
            "scipy==1.11.4\n",
            "scooby==0.10.0\n",
            "scs==3.2.4.post2\n",
            "seaborn==0.13.1\n",
            "SecretStorage==3.3.1\n",
            "Send2Trash==1.8.3\n",
            "sentencepiece==0.1.99\n",
            "setuptools==67.7.2\n",
            "shapely==2.0.4\n",
            "shellingham==1.5.4\n",
            "simple_parsing==0.1.5\n",
            "six==1.16.0\n",
            "sklearn-pandas==2.2.0\n",
            "smart-open==7.0.4\n",
            "sniffio==1.3.1\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.12.1\n",
            "soupsieve==2.5\n",
            "soxr==0.3.7\n",
            "spacy==3.7.5\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "Sphinx==5.0.2\n",
            "sphinxcontrib-applehelp==1.0.8\n",
            "sphinxcontrib-devhelp==1.0.6\n",
            "sphinxcontrib-htmlhelp==2.0.5\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==1.0.7\n",
            "sphinxcontrib-serializinghtml==1.1.10\n",
            "SQLAlchemy==2.0.30\n",
            "sqlglot==20.11.0\n",
            "sqlparse==0.5.0\n",
            "srsly==2.4.8\n",
            "stack-data==0.6.3\n",
            "stanio==0.5.0\n",
            "statsmodels==0.14.2\n",
            "StrEnum==0.4.15\n",
            "sympy==1.12.1\n",
            "tables==3.8.0\n",
            "tabulate==0.9.0\n",
            "tbb==2021.12.0\n",
            "tblib==3.0.0\n",
            "tenacity==8.4.1\n",
            "tensorboard==2.15.2\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorflow==2.15.0\n",
            "tensorflow-datasets==4.9.6\n",
            "tensorflow-estimator==2.15.0\n",
            "tensorflow-gcs-config==2.15.0\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-io-gcs-filesystem==0.37.0\n",
            "tensorflow-metadata==1.15.0\n",
            "tensorflow-probability==0.23.0\n",
            "tensorstore==0.1.45\n",
            "termcolor==2.4.0\n",
            "terminado==0.18.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.17.1\n",
            "textstat==0.7.3\n",
            "tf_keras==2.15.1\n",
            "tf-slim==1.1.0\n",
            "thinc==8.2.4\n",
            "threadpoolctl==3.5.0\n",
            "tifffile==2024.5.22\n",
            "tinycss2==1.3.0\n",
            "tokenizers==0.19.1\n",
            "toml==0.10.2\n",
            "tomli==2.0.1\n",
            "toolz==0.12.1\n",
            "torch==2.3.0+cu121\n",
            "torchaudio==2.3.0+cu121\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.18.0\n",
            "torchvision==0.18.0+cu121\n",
            "tornado==6.3.3\n",
            "tqdm==4.66.4\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.41.2\n",
            "triton==2.3.0\n",
            "tweepy==4.14.0\n",
            "typer==0.12.3\n",
            "types-pytz==2024.1.0.20240417\n",
            "types-setuptools==70.0.0.20240524\n",
            "typing_extensions==4.12.2\n",
            "tzdata==2024.1\n",
            "tzlocal==5.2\n",
            "uc-micro-py==1.0.3\n",
            "uritemplate==4.1.1\n",
            "urllib3==2.0.7\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wasabi==1.1.3\n",
            "wcwidth==0.2.13\n",
            "weasel==0.4.1\n",
            "webcolors==24.6.0\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.8.0\n",
            "Werkzeug==3.0.3\n",
            "wheel==0.43.0\n",
            "widgetsnbextension==3.6.6\n",
            "wordcloud==1.9.3\n",
            "wrapt==1.14.1\n",
            "xarray==2023.7.0\n",
            "xarray-einstats==0.7.0\n",
            "xgboost==2.0.3\n",
            "xlrd==2.0.1\n",
            "xyzservices==2024.6.0\n",
            "yarg==0.1.9\n",
            "yarl==1.9.4\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.40\n",
            "zict==3.0.0\n",
            "zipp==3.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list --format=freeze > initial_requirements.txt"
      ],
      "metadata": {
        "id": "52wUjUsH3vet"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list --format=freeze > current_requirements.txt"
      ],
      "metadata": {
        "id": "vQ4J8AKQ3zs7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!diff initial_requirements.txt current_requirements.txt"
      ],
      "metadata": {
        "id": "B4GdO01z33AU"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}